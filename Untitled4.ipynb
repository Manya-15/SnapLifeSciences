{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-google-genai langchain-community langchain_core langgraph pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LGxWBRUl555F",
        "outputId": "e0769028-41f6-4d34-b0b8-269d8237b1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.67)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.5.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.7)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.14.0)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.6-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.5.1-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, dataclasses-json, langchain_core, langgraph-checkpoint, google-ai-generativelanguage, langgraph-prebuilt, langchain-google-genai, langgraph, langchain-community\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.67\n",
            "    Uninstalling langchain-core-0.3.67:\n",
            "      Successfully uninstalled langchain-core-0.3.67\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.6.18 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-google-genai-2.1.6 langchain_core-0.3.68 langgraph-0.5.1 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.72 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.10.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "9bab7558c65d423982366131789468ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y58OTqK2MIxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d416b7-54b2-4de9-80d1-f791d4a7a246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 6 companies.\n",
            "Loaded 11 molecules.\n",
            "Loaded 3 external trial data entries.\n",
            "Loaded 4 market benchmarks.\n",
            "Loaded 5 literature snippets.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional, Literal, TypedDict\n",
        "from pydantic import BaseModel, Field, conint, confloat, ValidationError\n",
        "from enum import Enum\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# LangChain specific imports\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_core.documents import Document # Moved here for consistency\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Ensure your GOOGLE_API_KEY is correctly set.\n",
        "GOOGLE_API_KEY = \"AIzaSyC4q-ry8oPTjBHDP1suYrtB2PX52MXREwg\"\n",
        "\n",
        "LANGCHAIN_API_KEY = \"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\" # Optional, for Langsmith tracing\n",
        "LANGCHAIN_TRACING_V2 = \"true\" # Enable Langsmith for tracing!\n",
        "LANGCHAIN_PROJECT = \"Strategic Portfolio AI\"\n",
        "\n",
        "# --- 1. Enums and Pydantic Models for Data and Input/Output ---\n",
        "\n",
        "# ENUMS for fixed, categorical choices (UI-driven)\n",
        "class AssetType(str, Enum):\n",
        "    MOLECULE = \"Molecule\"\n",
        "    PLATFORM = \"Platform\"\n",
        "    DEVICE = \"Device\"\n",
        "\n",
        "class StrategicObjectiveEnum(str, Enum):\n",
        "    PRUNING = \"Pruning the Portfolio\"\n",
        "    DIVERSIFICATION = \"Diversification\"\n",
        "    FILLING_PIPELINE = \"Filling the Pipeline\"\n",
        "    AUGMENTING_PIPELINE = \"Augmenting the Pipeline / New Indication Space\"\n",
        "    CUSTOM_QUERY = \"Custom Query\"\n",
        "\n",
        "# Pydantic Models for Data (flexible string types for dynamic fields)\n",
        "class Molecule(BaseModel):\n",
        "    id: str\n",
        "    name: str\n",
        "    ndc_code: Optional[str] = None\n",
        "    asset_type: AssetType # Still an Enum\n",
        "    development_stage: str # Dynamic string\n",
        "    therapeutic_area: str # Dynamic string\n",
        "    indication: str\n",
        "    mechanism_of_action: str # Dynamic string\n",
        "    route_of_administration: str # Dynamic string\n",
        "    modality: str # Dynamic string\n",
        "    patent_expiry_year: int\n",
        "    current_roi: Optional[float] = None\n",
        "    projected_peak_sales_M: Optional[confloat(ge=0)] = None\n",
        "    internal_risk_score: confloat(ge=0, le=1)\n",
        "    efficacy_profile: str\n",
        "    safety_profile: str\n",
        "    is_for_sale: bool = False\n",
        "    company_id: str\n",
        "    company_name: str\n",
        "\n",
        "class Company(BaseModel):\n",
        "    id: str\n",
        "    name: str\n",
        "    type: str # Dynamic string\n",
        "    development_stage: str # Dynamic string\n",
        "    headquarters: str\n",
        "    financial_status: str # Dynamic string\n",
        "    partner_status: Optional[str] = None\n",
        "    territory: str\n",
        "\n",
        "class ExternalTrialData(BaseModel):\n",
        "    trial_id: str\n",
        "    molecule_id: str\n",
        "    indication: str\n",
        "    development_stage: str # Dynamic string\n",
        "    results_summary: str\n",
        "    safety_profile: str\n",
        "    efficacy_profile: str\n",
        "    target_biology: str\n",
        "    pathway_overlap: List[str] = Field(default_factory=list)\n",
        "    patient_population_characteristics: str\n",
        "\n",
        "class MarketBenchmark(BaseModel):\n",
        "    therapeutic_area: str # Dynamic string\n",
        "    development_stage: str # Dynamic string\n",
        "    avg_roi: float\n",
        "    avg_peak_sales_M: confloat(ge=0)\n",
        "    avg_time_in_stage_months: conint(gt=0)\n",
        "    success_rate: confloat(ge=0, le=1)\n",
        "\n",
        "# Frontend Input Model (Adapted to new data types for filters)\n",
        "class AssetPhenotypeFilters(BaseModel):\n",
        "    stages_of_development: Optional[List[str]] = None\n",
        "    therapeutic_area: Optional[List[str]] = None\n",
        "    indication: Optional[List[str]] = None\n",
        "    mechanism_of_action: Optional[List[str]] = None\n",
        "    route_of_administration: Optional[List[str]] = None\n",
        "    modality: Optional[List[str]] = None\n",
        "    patent_expiry_yr: Optional[conint(gt=1900, le=2100)] = None\n",
        "\n",
        "class CompanyDetailsFilters(BaseModel):\n",
        "    company_type: Optional[List[str]] = None\n",
        "    development_stage: Optional[List[str]] = None\n",
        "    headquarters: Optional[List[str]] = None\n",
        "    financial_status: Optional[List[str]] = None\n",
        "    partner_status: Optional[List[str]] = None\n",
        "    territory: Optional[List[str]] = None\n",
        "\n",
        "class PeakSalesFilters(BaseModel):\n",
        "    one_yr_sales_potential_M: Optional[confloat(ge=0)] = None\n",
        "    five_yr_sales_potential_M: Optional[confloat(ge=0)] = None\n",
        "    peak_sales_M: Optional[confloat(ge=0)] = None\n",
        "\n",
        "class FrontendFilters(BaseModel):\n",
        "    for_sale: Literal[\"all\", \"for_sale\", \"for_purchase\", \"sold\"] = \"all\"\n",
        "    asset_type: Optional[AssetType] = None\n",
        "    deal_value_min: Optional[confloat(ge=0)] = None\n",
        "    deal_value_max: Optional[confloat(ge=0)] = None\n",
        "    asset_phenotype: Optional[AssetPhenotypeFilters] = Field(default_factory=AssetPhenotypeFilters)\n",
        "    company_details: Optional[CompanyDetailsFilters] = Field(default_factory=CompanyDetailsFilters)\n",
        "    peak_sales: Optional[PeakSalesFilters] = Field(default_factory=PeakSalesFilters)\n",
        "    search_query: Optional[str] = None\n",
        "\n",
        "class FrontendInput(BaseModel):\n",
        "    filters: FrontendFilters = Field(default_factory=FrontendFilters)\n",
        "    strategic_objective: StrategicObjectiveEnum\n",
        "    custom_query_text: Optional[str] = None\n",
        "    current_company_id: str # This ID comes with the user input\n",
        "\n",
        "\n",
        "# Output Structures for Recommendations\n",
        "class PruningRecommendation(BaseModel):\n",
        "    action_type: Literal[\"Deprioritize Asset\"]\n",
        "    molecule_name: str\n",
        "    molecule_id: str\n",
        "    justification: str\n",
        "    reason_criteria: List[str]\n",
        "    risk_score: confloat(ge=0, le=1)\n",
        "    opportunity_cost_estimate: str\n",
        "    impact_on_portfolio: Dict[str, Any]\n",
        "\n",
        "class DiversificationRecommendation(BaseModel):\n",
        "    action_type: Literal[\"Acquire Asset\", \"Invest in Research Area\"]\n",
        "    molecule_name: Optional[str] = None\n",
        "    molecule_id: Optional[str] = None\n",
        "    reason_for_diversification: str\n",
        "    strategic_fit_score: confloat(ge=0, le=1)\n",
        "    target_disease_area: str # Dynamic string\n",
        "    proposed_moa: str # Dynamic string\n",
        "    proposed_modality: str # Dynamic string\n",
        "\n",
        "class FillingPipelineRecommendation(BaseModel):\n",
        "    action_type: Literal[\"Acquire Asset\", \"Initiate Internal Project\"]\n",
        "    molecule_name: str\n",
        "    molecule_id: str\n",
        "    reason_for_suggestion: str\n",
        "    suggested_role: str\n",
        "    development_stage_fit: str # Dynamic string\n",
        "\n",
        "class AugmentingPipelineRecommendation(BaseModel):\n",
        "    action_type: Literal[\"Initiate New Indication Trial\"]\n",
        "    molecule_name: str\n",
        "    molecule_id: str\n",
        "    new_indication: str\n",
        "    justification: str\n",
        "    evidence_strength: Literal[\"Low\", \"Moderate\", \"High\", \"Very High\"]\n",
        "    market_potential_estimate: str\n",
        "\n",
        "class StrategicOutput(BaseModel):\n",
        "    strategic_objective_addressed: StrategicObjectiveEnum\n",
        "    recommendations: List[Any] # Can be a list of any of the above recommendation types\n",
        "    strategic_summary: str\n",
        "    overall_impact_on_portfolio: Dict[str, Any]\n",
        "    recommended_portfolio_adjustments: Dict[str, Any]\n",
        "    suggested_ideal_portfolio_characteristics: Dict[str, Any]\n",
        "\n",
        "# --- Initialize Global LLM and Embeddings ---\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2, google_api_key=GOOGLE_API_KEY)\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "\n",
        "# --- 2. Data Management Layer ---\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self, data_file_path: str, embeddings_model: GoogleGenerativeAIEmbeddings):\n",
        "        self.data_file_path = data_file_path\n",
        "        self.embeddings = embeddings_model\n",
        "        self._load_all_data() # Load all data once at initialization\n",
        "\n",
        "        # Initialize these as None; they will be populated when set_current_company is called\n",
        "        self.internal_portfolio_vs: Optional[Chroma] = None\n",
        "        self.competitive_landscape_vs: Optional[Chroma] = None\n",
        "        self.external_molecule_database_vs: Optional[Chroma] = None\n",
        "        self.external_trial_data_vs: Optional[Chroma] = None\n",
        "        self.literature_vs: Optional[Chroma] = None\n",
        "\n",
        "        self._initialize_static_vector_stores()\n",
        "\n",
        "\n",
        "    def _initialize_static_vector_stores(self):\n",
        "        \"\"\"Initializes vector stores that do not depend on `your_company_id`.\"\"\"\n",
        "        # Initialize ChromaDB vector stores that don't depend on self.your_company_id\n",
        "        # The external_molecule_database_vs and external_trial_data_vs can be initialized with all data\n",
        "        self.external_molecule_database_vs = Chroma.from_documents(\n",
        "            documents=[doc for m in self.all_molecules.values() for doc in self._molecule_to_documents(m)] if self.all_molecules else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"external_molecule_database_v3\"\n",
        "        )\n",
        "        self.external_trial_data_vs = Chroma.from_documents(\n",
        "            documents=[doc for t in self.external_trial_data.values() for doc in self._trial_to_documents(t)] if self.external_trial_data else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"external_trial_data_v3\"\n",
        "        )\n",
        "        self.literature_vs = Chroma.from_texts(\n",
        "            texts=[s[\"text\"] for s in self.literature_snippets] if self.literature_snippets else [],\n",
        "            metadatas=[{\"source\": s[\"source\"]} for s in self.literature_snippets] if self.literature_snippets else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"literature_snippets_v3\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def _load_all_data(self):\n",
        "        \"\"\"Loads and parses all data from the single JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(self.data_file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            self.all_companies: Dict[str, Company] = {c[\"id\"]: Company(**c) for c in data.get(\"companies\", [])}\n",
        "            self.all_molecules: Dict[str, Molecule] = {m[\"id\"]: Molecule(**m) for m in data.get(\"molecules\", [])}\n",
        "            self.external_trial_data: Dict[str, ExternalTrialData] = {t[\"trial_id\"]: ExternalTrialData(**t) for t in data.get(\"external_trial_data\", [])}\n",
        "            self.market_benchmarks: List[MarketBenchmark] = [MarketBenchmark(**bm) for bm in data.get(\"market_benchmarks\", [])]\n",
        "            self.literature_snippets: List[Dict[str,str]] = data.get(\"literature_snippets\", [])\n",
        "\n",
        "            print(f\"Loaded {len(self.all_companies)} companies.\")\n",
        "            print(f\"Loaded {len(self.all_molecules)} molecules.\")\n",
        "            print(f\"Loaded {len(self.external_trial_data)} external trial data entries.\")\n",
        "            print(f\"Loaded {len(self.market_benchmarks)} market benchmarks.\")\n",
        "            print(f\"Loaded {len(self.literature_snippets)} literature snippets.\")\n",
        "\n",
        "            # These will be set dynamically based on user input\n",
        "            self.your_company_id: Optional[str] = None\n",
        "            self.your_company_profile: Optional[Company] = None\n",
        "            self.internal_molecules: Dict[str, Molecule] = {}\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Data file not found at: {self.data_file_path}\")\n",
        "        except ValidationError as e:\n",
        "            print(f\"Pydantic validation error when loading data: {e.errors()}\")\n",
        "            raise\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(f\"Error decoding JSON from {self.data_file_path}. Ensure it's valid JSON.\")\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Missing expected key in JSON data: {e}. Check your JSON structure.\")\n",
        "\n",
        "    def set_current_company(self, company_id: str):\n",
        "        \"\"\"Sets the current company for the session and initializes company-specific vector stores.\"\"\"\n",
        "        self.your_company_id = company_id\n",
        "        self.your_company_profile = self.all_companies.get(company_id)\n",
        "        self.internal_molecules = {\n",
        "            mol.id: mol for mol in self.all_molecules.values() if mol.company_id == company_id\n",
        "        }\n",
        "        if not self.your_company_profile:\n",
        "            print(f\"Warning: Current company ID '{company_id}' not found in loaded data. Internal portfolio will be empty.\")\n",
        "\n",
        "        # Initialize internal_portfolio_vs and competitive_landscape_vs here\n",
        "        self.internal_portfolio_vs = Chroma.from_documents(\n",
        "            documents=[doc for m in self.internal_molecules.values() for doc in self._molecule_to_documents(m)] if self.internal_molecules else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"internal_portfolio_v3\"\n",
        "        )\n",
        "        self.competitive_landscape_vs = Chroma.from_documents(\n",
        "            documents=[doc for m in self.all_molecules.values() if m.company_id != self.your_company_id for doc in self._molecule_to_documents(m)] if self.all_molecules else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"competitive_landscape_v3\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def _molecule_to_documents(self, molecule: Molecule) -> List[Document]:\n",
        "        \"\"\"Converts a Molecule Pydantic model to LangChain Document(s).\"\"\"\n",
        "        content = (\n",
        "            f\"Molecule Name: {molecule.name}\\n\"\n",
        "            f\"ID: {molecule.id}\\n\"\n",
        "            f\"Type: {molecule.asset_type.value}\\n\"\n",
        "            f\"Stage: {molecule.development_stage}\\n\"\n",
        "            f\"Therapeutic Area: {molecule.therapeutic_area}\\n\"\n",
        "            f\"Indication: {molecule.indication}\\n\"\n",
        "            f\"Mechanism of Action: {molecule.mechanism_of_action}\\n\"\n",
        "            f\"Modality: {molecule.modality}\\n\"\n",
        "            f\"Patent Expiry: {molecule.patent_expiry_year}\\n\"\n",
        "            f\"Projected Peak Sales: ${molecule.projected_peak_sales_M or 'N/A'}M\\n\"\n",
        "            f\"Internal Risk Score: {molecule.internal_risk_score}\\n\"\n",
        "            f\"Efficacy: {molecule.efficacy_profile}\\n\"\n",
        "            f\"Safety: {molecule.safety_profile}\\n\"\n",
        "            f\"For Sale: {molecule.is_for_sale}\\n\"\n",
        "            f\"Company: {molecule.company_name} ({molecule.company_id})\"\n",
        "        )\n",
        "        metadata = molecule.model_dump()\n",
        "        metadata[\"asset_type\"] = metadata[\"asset_type\"].value # Ensure enum is string\n",
        "        return [Document(page_content=content, metadata=metadata)]\n",
        "\n",
        "    def _trial_to_documents(self, trial: ExternalTrialData) -> List[Document]:\n",
        "        \"\"\"Converts an ExternalTrialData Pydantic model to LangChain Document(s).\"\"\"\n",
        "        content = (\n",
        "            f\"Trial ID: {trial.trial_id}\\n\"\n",
        "            f\"Molecule ID: {trial.molecule_id}\\n\"\n",
        "            f\"Indication: {trial.indication}\\n\"\n",
        "            f\"Stage: {trial.development_stage}\\n\"\n",
        "            f\"Results: {trial.results_summary}\\n\"\n",
        "            f\"Safety: {trial.safety_profile}\\n\"\n",
        "            f\"Efficacy: {trial.efficacy_profile}\\n\"\n",
        "            f\"Target Biology: {trial.target_biology}\\n\"\n",
        "            f\"Pathway Overlap: {', '.join(trial.pathway_overlap)}\\n\" # FIX: Join list into string\n",
        "            f\"Patient Population: {trial.patient_population_characteristics}\"\n",
        "        )\n",
        "        metadata = trial.model_dump()\n",
        "        # FIX: Convert pathway_overlap list to a comma-separated string for metadata\n",
        "        metadata[\"pathway_overlap\"] = \", \".join(trial.pathway_overlap)\n",
        "        return [Document(page_content=content, metadata=metadata)]\n",
        "\n",
        "    def get_molecule_by_id(self, molecule_id: str) -> Optional[Molecule]:\n",
        "        return self.all_molecules.get(molecule_id)\n",
        "\n",
        "    def get_company_by_id(self, company_id: str) -> Optional[Company]:\n",
        "        return self.all_companies.get(company_id)\n",
        "\n",
        "    def get_market_benchmark(self, ta: str, ds: str) -> Optional[MarketBenchmark]:\n",
        "        for bm in self.market_benchmarks:\n",
        "            if bm.therapeutic_area.lower() == ta.lower() and bm.development_stage.lower() == ds.lower():\n",
        "                return bm\n",
        "        return None\n",
        "\n",
        "    def get_internal_portfolio(self) -> List[Molecule]:\n",
        "        return list(self.internal_molecules.values())\n",
        "\n",
        "    def get_all_molecules(self) -> List[Molecule]:\n",
        "        return list(self.all_molecules.values())\n",
        "\n",
        "    def get_filtered_molecules(self, molecules: List[Molecule], filters: FrontendFilters) -> List[Molecule]:\n",
        "        \"\"\"\n",
        "        Splitting this function into several smaller parts:\n",
        "        - _apply_asset_filters\n",
        "        - _apply_deal_value_filters\n",
        "        - _apply_phenotype_filters\n",
        "        - _apply_company_filters\n",
        "        \"\"\"\n",
        "        filtered_list = []\n",
        "        for mol in molecules:\n",
        "            # Apply 'for_sale' filter\n",
        "            if filters.for_sale == \"for_sale\" and not mol.is_for_sale:\n",
        "                continue\n",
        "            if filters.for_sale == \"for_purchase\" and (not mol.is_for_sale or mol.company_id == self.your_company_id):\n",
        "                continue\n",
        "            # \"sold\" is a historical state, not a current filter on available assets\n",
        "\n",
        "            # Apply asset_type filter\n",
        "            if filters.asset_type and mol.asset_type != filters.asset_type:\n",
        "                continue\n",
        "\n",
        "            # Apply deal value filters (based on projected peak sales)\n",
        "            if filters.deal_value_min is not None and (mol.projected_peak_sales_M is None or mol.projected_peak_sales_M < filters.deal_value_min):\n",
        "                continue\n",
        "            if filters.deal_value_max is not None and (mol.projected_peak_sales_M is None or mol.projected_peak_sales_M > filters.deal_value_max):\n",
        "                continue\n",
        "            if filters.peak_sales:\n",
        "                if filters.peak_sales.peak_sales_M is not None and (mol.projected_peak_sales_M is None or mol.projected_peak_sales_M < filters.peak_sales.peak_sales_M):\n",
        "                    continue\n",
        "                # 1yr/5yr sales are not in Molecule model, skip for now.\n",
        "\n",
        "            # Apply AssetPhenotypeFilters\n",
        "            if filters.asset_phenotype:\n",
        "                if filters.asset_phenotype.stages_of_development and mol.development_stage not in filters.asset_phenotype.stages_of_development:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.therapeutic_area and mol.therapeutic_area not in filters.asset_phenotype.therapeutic_area:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.indication and mol.indication not in filters.asset_phenotype.indication:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.mechanism_of_action and mol.mechanism_of_action not in filters.asset_phenotype.mechanism_of_action:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.route_of_administration and mol.route_of_administration not in filters.asset_phenotype.route_of_administration:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.modality and mol.modality not in filters.asset_phenotype.modality:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.patent_expiry_yr is not None and mol.patent_expiry_year < filters.asset_phenotype.patent_expiry_yr:\n",
        "                    continue\n",
        "\n",
        "            # Apply CompanyDetailsFilters (requires looking up company info)\n",
        "            if filters.company_details:\n",
        "                company_of_mol = self.get_company_by_id(mol.company_id)\n",
        "                if company_of_mol:\n",
        "                    if filters.company_details.company_type and company_of_mol.type not in filters.company_details.company_type:\n",
        "                        continue\n",
        "                    if filters.company_details.development_stage and company_of_mol.development_stage not in filters.company_details.development_stage:\n",
        "                        continue\n",
        "                    if filters.company_details.headquarters and company_of_mol.headquarters not in filters.company_details.headquarters:\n",
        "                        continue\n",
        "                    if filters.company_details.financial_status and company_of_mol.financial_status not in filters.company_details.financial_status:\n",
        "                        continue\n",
        "                    if filters.company_details.partner_status and company_of_mol.partner_status not in filters.company_details.partner_status:\n",
        "                        continue\n",
        "                    if filters.company_details.territory and company_of_mol.territory not in filters.company_details.territory:\n",
        "                        continue\n",
        "                else: # If company data for molecule is missing or doesn't exist, it doesn't match\n",
        "                    continue\n",
        "\n",
        "            filtered_list.append(mol)\n",
        "        return filtered_list\n",
        "\n",
        "# Instantiate DataManager (assuming 'company_data.json' exists in the same directory)\n",
        "data_manager = DataManager('/content/company.json', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. LangChain Tools for Strategic Objectives ---\n",
        "# These tools use the data_manager and LLM to perform specific analyses.\n",
        "\n",
        "class StrategicTools:\n",
        "    def __init__(self, llm: ChatGoogleGenerativeAI, data_manager: DataManager):\n",
        "        self.llm = llm\n",
        "        self.data_manager = data_manager\n",
        "\n",
        "    @tool\n",
        "    def analyze_pruning_portfolio(self, filters: FrontendFilters, current_company_id: str) -> List[PruningRecommendation]:\n",
        "        \"\"\"\n",
        "        Analyzes the internal portfolio to identify assets for deprioritization or pruning.\n",
        "        Considers factors like low ROI, high risk, expiring patents, competition, and strategic misalignment.\n",
        "        Returns a list of PruningRecommendation objects.\n",
        "        \"\"\"\n",
        "        self.data_manager.set_current_company(current_company_id) # Ensure current company context is set\n",
        "        your_molecules = self.data_manager.get_filtered_molecules(self.data_manager.get_internal_portfolio(), filters)\n",
        "\n",
        "        # Get competitive landscape, potentially applying the same filters for relevance\n",
        "        all_other_molecules = [mol for mol in self.data_manager.get_all_molecules() if mol.company_id != current_company_id]\n",
        "        competitive_molecules = self.data_manager.get_filtered_molecules(all_other_molecules, filters)\n",
        "\n",
        "        internal_portfolio_context = \"\\n\".join([mol.model_dump_json() for mol in your_molecules])\n",
        "        competitive_context = \"\\n\".join([mol.model_dump_json() for mol in competitive_molecules])\n",
        "        market_benchmarks_context = \"\\n\".join([bm.model_dump_json() for bm in self.data_manager.market_benchmarks])\n",
        "\n",
        "        pruning_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an expert pharmaceutical portfolio manager. Your goal is to identify molecules in the internal portfolio that should be deprioritized or pruned.\n",
        "             Consider the following criteria for pruning:\n",
        "             - Low projected peak sales or current ROI.\n",
        "             - High internal risk score.\n",
        "             - Soon-to-expire patents (e.g., within 5 years).\n",
        "             - Presence of superior competitive alternatives.\n",
        "             - Misalignment with current strategic objectives (though the objective here is pruning).\n",
        "             - Limited market potential or high development costs for potential return.\n",
        "\n",
        "             Analyze the provided internal portfolio data and competitive landscape to make informed recommendations.\n",
        "             Output your recommendations strictly as a JSON list of PruningRecommendation objects. Each object must conform to the PruningRecommendation Pydantic schema.\"\"\"),\n",
        "            (\"human\", f\"\"\"Current Internal Portfolio (filtered for your company {current_company_id}):\n",
        "             <internal_portfolio>\n",
        "             {internal_portfolio_context}\n",
        "             </internal_portfolio>\n",
        "\n",
        "             Competitive Landscape (filtered):\n",
        "             <competitive_landscape>\n",
        "             {competitive_context}\n",
        "             </competitive_landscape>\n",
        "\n",
        "             Market Benchmarks:\n",
        "             <market_benchmarks>\n",
        "             {market_benchmarks_context}\n",
        "             </market_benchmarks>\n",
        "\n",
        "             My company ID is: {current_company_id}\n",
        "\n",
        "             Identify molecules for pruning and provide detailed justifications based on the above data.\n",
        "             Output a JSON list of PruningRecommendation objects.\"\"\")\n",
        "        ])\n",
        "\n",
        "        # Using a custom parser for list of Pydantic models as structured_output doesn't directly support List[Model]\n",
        "        # and we want to ensure robust parsing.\n",
        "        class PruningList(BaseModel):\n",
        "            recommendations: List[PruningRecommendation]\n",
        "\n",
        "        chain = pruning_prompt | self.llm.with_structured_output(PruningList)\n",
        "\n",
        "        try:\n",
        "            raw_output = chain.invoke({}) # No explicit input dict needed for chain when using .invoke directly with prompt\n",
        "            return raw_output.recommendations\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error in pruning tool output: {e.errors()}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error in pruning tool: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    @tool\n",
        "    def analyze_diversification(self, filters: FrontendFilters, current_company_id: str) -> List[DiversificationRecommendation]:\n",
        "        \"\"\"\n",
        "        Identifies new therapeutic areas, MOAs, or modalities for portfolio diversification.\n",
        "        Considers current portfolio gaps, emerging trends, and high-potential external assets.\n",
        "        Returns a list of DiversificationRecommendation objects.\n",
        "        \"\"\"\n",
        "        self.data_manager.set_current_company(current_company_id)\n",
        "        your_molecules = self.data_manager.get_internal_portfolio()\n",
        "\n",
        "        all_external_molecules = [mol for mol in self.data_manager.get_all_molecules() if mol.company_id != current_company_id]\n",
        "        external_molecules_filtered = self.data_manager.get_filtered_molecules(all_external_molecules, filters)\n",
        "\n",
        "        literature_snippets = self.data_manager.literature_vs.similarity_search(\n",
        "            filters.search_query if filters.search_query else \"emerging therapeutic areas OR novel mechanisms of action OR new modalities\", k=5\n",
        "        )\n",
        "\n",
        "        internal_portfolio_context = \"\\n\".join([mol.model_dump_json() for mol in your_molecules])\n",
        "        external_molecules_context = \"\\n\".join([mol.model_dump_json() for mol in external_molecules_filtered])\n",
        "        literature_context = \"\\n\".join([doc.page_content for doc in literature_snippets])\n",
        "\n",
        "        diversification_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an expert pharmaceutical portfolio strategist. Your objective is to recommend new areas for portfolio diversification.\n",
        "             Consider the current internal portfolio, the competitive landscape (external assets), and recent scientific literature.\n",
        "             Look for:\n",
        "             - Gaps in the current portfolio (e.g., unaddressed therapeutic areas, MOAs, or modalities).\n",
        "             - Emerging research trends or novel technologies.\n",
        "             - Promising external assets that align with diversification goals.\n",
        "             - Areas with high unmet medical need.\n",
        "\n",
        "             Output your recommendations strictly as a JSON list of DiversificationRecommendation objects. Each object must conform to the DiversificationRecommendation Pydantic schema.\"\"\"),\n",
        "            (\"human\", f\"\"\"My current internal portfolio:\n",
        "             <internal_portfolio>\n",
        "             {internal_portfolio_context}\n",
        "             </internal_portfolio>\n",
        "\n",
        "             Relevant external assets and competitive landscape (filtered):\n",
        "             <external_molecules>\n",
        "             {external_molecules_context}\n",
        "             </external_molecules>\n",
        "\n",
        "             Recent scientific literature and trends:\n",
        "             <literature>\n",
        "             {literature_context}\n",
        "             </literature>\n",
        "\n",
        "             My company ID is: {current_company_id}\n",
        "\n",
        "             Based on this information, suggest strategies for portfolio diversification.\n",
        "             Output a JSON list of DiversificationRecommendation objects.\"\"\")\n",
        "        ])\n",
        "\n",
        "        class DiversificationList(BaseModel):\n",
        "            recommendations: List[DiversificationRecommendation]\n",
        "\n",
        "        chain = diversification_prompt | self.llm.with_structured_output(DiversificationList)\n",
        "\n",
        "        try:\n",
        "            raw_output = chain.invoke({})\n",
        "            return raw_output.recommendations\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error in diversification tool output: {e.errors()}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error in diversification tool: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    @tool\n",
        "    def analyze_filling_pipeline(self, filters: FrontendFilters, current_company_id: str) -> List[FillingPipelineRecommendation]:\n",
        "        \"\"\"\n",
        "        Identifies potential assets or research areas to fill pipeline gaps,\n",
        "        focusing on specific development stages or therapeutic areas.\n",
        "        Returns a list of FillingPipelineRecommendation objects.\n",
        "        \"\"\"\n",
        "        self.data_manager.set_current_company(current_company_id)\n",
        "        your_molecules = self.data_manager.get_internal_portfolio()\n",
        "\n",
        "        all_external_molecules = [mol for mol in self.data_manager.get_all_molecules() if mol.company_id != current_company_id]\n",
        "        external_molecules_for_purchase = self.data_manager.get_filtered_molecules(all_external_molecules, filters)\n",
        "\n",
        "        internal_portfolio_context = \"\\n\".join([mol.model_dump_json() for mol in your_molecules])\n",
        "        external_molecules_context = \"\\n\".join([mol.model_dump_json() for mol in external_molecules_for_purchase])\n",
        "\n",
        "        filling_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an expert pharmaceutical pipeline developer. Your task is to identify and recommend assets or research areas to fill specific gaps in the current internal pipeline.\n",
        "             Focus on opportunities that align with current development stages, therapeutic areas, or modalities that require strengthening.\n",
        "             Prioritize external assets that are 'for sale' and show a good strategic fit.\n",
        "             Output your recommendations strictly as a JSON list of FillingPipelineRecommendation objects. Each object must conform to the FillingPipelineRecommendation Pydantic schema.\"\"\"),\n",
        "            (\"human\", f\"\"\"My current internal portfolio:\n",
        "             <internal_portfolio>\n",
        "             {internal_portfolio_context}\n",
        "             </internal_portfolio>\n",
        "\n",
        "             Available external molecules (filtered by user criteria and 'for_sale' status):\n",
        "             <external_molecules>\n",
        "             {external_molecules_context}\n",
        "             </external_molecules>\n",
        "\n",
        "             My company ID is: {current_company_id}\n",
        "\n",
        "             Based on this, recommend assets or internal projects to fill pipeline gaps.\n",
        "             Output a JSON list of FillingPipelineRecommendation objects.\"\"\")\n",
        "        ])\n",
        "\n",
        "        class FillingList(BaseModel):\n",
        "            recommendations: List[FillingPipelineRecommendation]\n",
        "\n",
        "        chain = filling_prompt | self.llm.with_structured_output(FillingList)\n",
        "\n",
        "        try:\n",
        "            raw_output = chain.invoke({})\n",
        "            return raw_output.recommendations\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error in filling pipeline tool output: {e.errors()}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error in filling pipeline tool: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    @tool\n",
        "    def analyze_augmenting_pipeline(self, filters: FrontendFilters, current_company_id: str) -> List[AugmentingPipelineRecommendation]:\n",
        "        \"\"\"\n",
        "        Suggests new indications or expanded use for existing internal pipeline assets.\n",
        "        Leverages external trial data, scientific literature, and mechanistic overlaps.\n",
        "        Returns a list of AugmentingPipelineRecommendation objects.\n",
        "        \"\"\"\n",
        "        self.data_manager.set_current_company(current_company_id)\n",
        "        your_molecules = self.data_manager.get_filtered_molecules(self.data_manager.get_internal_portfolio(), filters)\n",
        "\n",
        "        all_trials = list(self.data_manager.external_trial_data.values())\n",
        "        # Filter trials relevant to the query/molecules. This is a simplified approach,\n",
        "        # in reality, you might need more sophisticated trial filtering based on keywords etc.\n",
        "        relevant_trials = []\n",
        "        for trial in all_trials:\n",
        "            # Simple keyword matching for demonstration\n",
        "            if filters.search_query and filters.search_query.lower() in trial.indication.lower():\n",
        "                relevant_trials.append(trial)\n",
        "            # Or if the trial's molecule ID is in our filtered internal molecules\n",
        "            if trial.molecule_id in [mol.id for mol in your_molecules]:\n",
        "                relevant_trials.append(trial)\n",
        "\n",
        "        literature_snippets = self.data_manager.literature_vs.similarity_search(\n",
        "            filters.search_query if filters.search_query else \"drug repurposing OR new indications OR mechanistic insights\", k=5\n",
        "        )\n",
        "\n",
        "        internal_portfolio_context = \"\\n\".join([mol.model_dump_json() for mol in your_molecules])\n",
        "        external_trial_context = \"\\n\".join([trial.model_dump_json() for trial in relevant_trials])\n",
        "        literature_context = \"\\n\".join([doc.page_content for doc in literature_snippets])\n",
        "\n",
        "        augmenting_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an expert in clinical development and drug repurposing. Your goal is to identify existing internal pipeline assets that could be augmented with new indications or expanded uses.\n",
        "             Consider the following:\n",
        "             - Mechanistic overlap with other diseases.\n",
        "             - Promising clinical trial data from similar compounds or targets.\n",
        "             - Relevant scientific literature suggesting new applications.\n",
        "             - Unmet medical needs in related disease areas.\n",
        "\n",
        "             Output your recommendations strictly as a JSON list of AugmentingPipelineRecommendation objects. Each object must conform to the AugmentingPipelineRecommendation Pydantic schema.\"\"\"),\n",
        "            (\"human\", f\"\"\"My current internal portfolio (filtered for your company {current_company_id}):\n",
        "             <internal_portfolio>\n",
        "             {internal_portfolio_context}\n",
        "             </internal_portfolio>\n",
        "\n",
        "             Relevant external trial data:\n",
        "             <external_trial_data>\n",
        "             {external_trial_context}\n",
        "             </external_trial_data>\n",
        "\n",
        "             Recent scientific literature:\n",
        "             <literature>\n",
        "             {literature_context}\n",
        "             </literature>\n",
        "\n",
        "             My company ID is: {current_company_id}\n",
        "\n",
        "             Suggest new indications for existing pipeline assets.\n",
        "             Output a JSON list of AugmentingPipelineRecommendation objects.\"\"\")\n",
        "        ])\n",
        "\n",
        "        class AugmentingList(BaseModel):\n",
        "            recommendations: List[AugmentingPipelineRecommendation]\n",
        "\n",
        "        chain = augmenting_prompt | self.llm.with_structured_output(AugmentingList)\n",
        "\n",
        "        try:\n",
        "            raw_output = chain.invoke({})\n",
        "            return raw_output.recommendations\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error in augmenting tool output: {e.errors()}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error in augmenting tool: {e}\")\n",
        "            return []\n",
        "\n",
        "# Instantiate the tools globally accessible\n",
        "strategic_tools = StrategicTools(llm, data_manager)\n",
        "tools = [\n",
        "    strategic_tools.analyze_pruning_portfolio,\n",
        "    strategic_tools.analyze_diversification,\n",
        "    strategic_tools.analyze_filling_pipeline,\n",
        "    strategic_tools.analyze_augmenting_pipeline,\n",
        "]\n",
        "\n",
        "\n",
        "# --- 4. LangGraph State and Workflow Definition ---\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "    \"\"\"\n",
        "    messages: List[BaseMessage]\n",
        "    user_input: FrontendInput\n",
        "    tool_output: Optional[List[Any]] # Can be a list of any recommendation type\n",
        "    strategic_output: Optional[StrategicOutput]\n",
        "    next_action: Literal[\"call_tool\", \"generate_final_response\", \"error\"]\n",
        "\n",
        "\n",
        "# Define the nodes (functions) that will operate on the state\n",
        "\n",
        "def call_strategic_tool_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Calls the appropriate strategic tool based on the user's objective.\n",
        "    \"\"\"\n",
        "    user_input = state[\"user_input\"]\n",
        "    strategic_objective = user_input.strategic_objective\n",
        "    filters = user_input.filters\n",
        "    current_company_id = user_input.current_company_id\n",
        "\n",
        "    tool_map = {\n",
        "        StrategicObjectiveEnum.PRUNING: strategic_tools.analyze_pruning_portfolio,\n",
        "        StrategicObjectiveEnum.DIVERSIFICATION: strategic_tools.analyze_diversification,\n",
        "        StrategicObjectiveEnum.FILLING_PIPELINE: strategic_tools.analyze_filling_pipeline,\n",
        "        StrategicObjectiveEnum.AUGMENTING_PIPELINE: strategic_tools.analyze_augmenting_pipeline,\n",
        "        # CUSTOM_QUERY handled by decision logic below if direct tool not applicable\n",
        "    }\n",
        "\n",
        "    tool_function = tool_map.get(strategic_objective)\n",
        "\n",
        "    if strategic_objective == StrategicObjectiveEnum.CUSTOM_QUERY:\n",
        "        # For custom query, we might just use the LLM to provide a general response\n",
        "        # or it could dynamically pick a tool if the query implies one.\n",
        "        # For this simplified flow, let's have it generate a general response.\n",
        "        print(f\"Custom Query detected. Generating general response.\")\n",
        "        return {**state, \"next_action\": \"generate_final_response\", \"tool_output\": []} # No tool output for custom query for now\n",
        "\n",
        "    if tool_function:\n",
        "        print(f\"Calling tool: {tool_function.__name__} for objective: {strategic_objective.value}\")\n",
        "        try:\n",
        "            result = tool_function(filters=filters, current_company_id=current_company_id)\n",
        "            print(f\"Tool {tool_function.__name__} returned {len(result)} recommendations.\")\n",
        "            return {**state, \"tool_output\": result, \"next_action\": \"generate_final_response\"}\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing tool {tool_function.__name__}: {e}\")\n",
        "            return {**state, \"next_action\": \"error\", \"messages\": state[\"messages\"] + [AIMessage(content=f\"Error executing strategic analysis: {e}\")]}\n",
        "    else:\n",
        "        print(f\"No specific tool found for objective: {strategic_objective.value}.\")\n",
        "        # Fallback for unexpected objectives or if tool_map isn't exhaustive\n",
        "        return {**state, \"next_action\": \"error\", \"messages\": state[\"messages\"] + [AIMessage(content=f\"Internal error: Strategic objective '{strategic_objective.value}' not mapped to a tool.\")]}\n",
        "\n",
        "\n",
        "def generate_final_response_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Generates the final structured StrategicOutput based on tool output or custom query.\n",
        "    \"\"\"\n",
        "    user_input = state[\"user_input\"]\n",
        "    tool_output = state[\"tool_output\"]\n",
        "    strategic_objective = user_input.strategic_objective\n",
        "\n",
        "    if strategic_objective == StrategicObjectiveEnum.CUSTOM_QUERY:\n",
        "        # Handle custom query directly with LLM to provide an answer\n",
        "        custom_query_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an expert pharmaceutical industry analyst. The user has a custom query.\n",
        "             Provide a comprehensive and insightful response based on the available data, even if it's general.\n",
        "             If the query is too broad or requires information not in the data, state that gracefully.\n",
        "             The output should still adhere to the StrategicOutput Pydantic model, even if 'recommendations' is empty.\n",
        "             Focus on the 'strategic_summary' for the main answer.\n",
        "             \"\"\"),\n",
        "            (\"human\", f\"\"\"User's Custom Query: {user_input.custom_query_text or user_input.filters.search_query}\n",
        "             Relevant filters applied: {user_input.filters.model_dump_json(exclude_none=True)}\n",
        "             Current Company ID: {user_input.current_company_id}\n",
        "\n",
        "             Provide a strategic analysis for this custom query.\"\"\")\n",
        "        ])\n",
        "\n",
        "        # We need to explicitly make the LLM output the StrategicOutput model\n",
        "        final_response_chain = custom_query_prompt | llm.with_structured_output(StrategicOutput)\n",
        "\n",
        "        try:\n",
        "            # We don't have tool_output for custom query, so pass empty recommendations for schema adherence\n",
        "            strategic_output_obj = final_response_chain.invoke({})\n",
        "            # Ensure recommendations are an empty list if not explicitly generated\n",
        "            if not strategic_output_obj.recommendations:\n",
        "                 strategic_output_obj.recommendations = []\n",
        "            print(f\"Generated final strategic output for custom query.\")\n",
        "            return {**state, \"strategic_output\": strategic_output_obj, \"next_action\": \"finished\"}\n",
        "\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error for custom query response: {e.errors()}\")\n",
        "            return {**state, \"next_action\": \"error\", \"messages\": state[\"messages\"] + [AIMessage(content=f\"Error structuring custom query response: {e}\")]}\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating custom query response: {e}\")\n",
        "            return {**state, \"next_action\": \"error\", \"messages\": state[\"messages\"] + [AIMessage(content=f\"Error processing custom query: {e}\")]}\n",
        "\n",
        "    # For all other strategic objectives, synthesize from tool_output\n",
        "    else:\n",
        "        if not tool_output:\n",
        "            summary_content = f\"No specific recommendations could be generated for {strategic_objective.value} based on the provided filters and data.\"\n",
        "            recommendations_list = []\n",
        "        else:\n",
        "            summary_content = f\"Recommendations for '{strategic_objective.value}' objective:\\n\"\n",
        "            recommendations_list = tool_output\n",
        "            for rec in tool_output:\n",
        "                summary_content += f\"- {rec.model_dump_json(indent=2)}\\n\"\n",
        "\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an executive assistant for a pharmaceutical company. Your task is to synthesize the strategic recommendations\n",
        "             provided by the analytical tools into a comprehensive and actionable StrategicOutput report.\n",
        "\n",
        "             Summarize the key findings, provide an overall strategic summary, detail any recommended portfolio adjustments,\n",
        "             and suggest characteristics of an ideal future portfolio if relevant.\n",
        "\n",
        "             Output the response strictly in the JSON format defined by the StrategicOutput Pydantic model.\n",
        "             If no specific recommendations were generated by the tool, explain why and provide a general strategic insight.\"\"\"),\n",
        "            (\"human\", f\"\"\"User's Strategic Objective: {strategic_objective.value}\n",
        "             User's Filters: {user_input.filters.model_dump_json(exclude_none=True)}\n",
        "\n",
        "             Analysis Results from Tools (raw recommendations):\n",
        "             <tool_results>\n",
        "             {json.dumps([rec.model_dump() for rec in recommendations_list], indent=2)}\n",
        "             </tool_results>\n",
        "\n",
        "             Please generate the final StrategicOutput report.\"\"\")\n",
        "        ])\n",
        "\n",
        "        final_response_chain = summary_prompt | llm.with_structured_output(StrategicOutput)\n",
        "\n",
        "        try:\n",
        "            strategic_output_obj = final_response_chain.invoke({})\n",
        "            # Ensure recommendations are properly set, even if the LLM sometimes puts them in summary\n",
        "            strategic_output_obj.recommendations = recommendations_list\n",
        "            print(f\"Generated final strategic output for objective: {strategic_objective.value}\")\n",
        "            return {**state, \"strategic_output\": strategic_output_obj, \"next_action\": \"finished\"}\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error generating final response: {e.errors()}\")\n",
        "            return {**state, \"next_action\": \"error\", \"messages\": state[\"messages\"] + [AIMessage(content=f\"Error structuring final report: {e}\")]}\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating final response: {e}\")\n",
        "            return {**state, \"next_action\": \"error\", \"messages\": state[\"messages\"] + [AIMessage(content=f\"Error generating final strategic report: {e}\")]}\n",
        "\n",
        "\n",
        "def handle_error_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Handles errors encountered during graph execution.\"\"\"\n",
        "    print(f\"Error state reached. Messages: {state['messages']}\")\n",
        "    error_message = state['messages'][-1].content if state['messages'] else 'Unknown error'\n",
        "    return {**state, \"strategic_output\": StrategicOutput(\n",
        "        strategic_objective_addressed=state[\"user_input\"].strategic_objective,\n",
        "        recommendations=[],\n",
        "        strategic_summary=f\"An error occurred during analysis for '{state['user_input'].strategic_objective.value}': {error_message}\",\n",
        "        overall_impact_on_portfolio={},\n",
        "        recommended_portfolio_adjustments={},\n",
        "        suggested_ideal_portfolio_characteristics={}\n",
        "    ), \"next_action\": \"finished\"}\n",
        "\n",
        "\n",
        "# --- 5. Build the LangGraph Workflow ---\n",
        "\n",
        "def build_strategic_pipeline_graph():\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    # Add nodes for tool calling and response generation\n",
        "    workflow.add_node(\"call_tool\", call_strategic_tool_node)\n",
        "    workflow.add_node(\"generate_response\", generate_final_response_node)\n",
        "    workflow.add_node(\"error_handler\", handle_error_node)\n",
        "\n",
        "    # Set the entry point\n",
        "    workflow.set_entry_point(\"call_tool\")\n",
        "\n",
        "    # Define the edges (transitions)\n",
        "    workflow.add_conditional_edges(\n",
        "        \"call_tool\",\n",
        "        lambda state: state[\"next_action\"],\n",
        "        {\n",
        "            \"generate_final_response\": \"generate_response\",\n",
        "            \"error\": \"error_handler\"\n",
        "        }\n",
        "    )\n",
        "    workflow.add_conditional_edges(\n",
        "        \"generate_response\",\n",
        "        lambda state: \"finished\" if state.get(\"strategic_output\") else \"error\",\n",
        "        {\n",
        "            \"finished\": END,\n",
        "            \"error\": \"error_handler\"\n",
        "        }\n",
        "    )\n",
        "    workflow.add_edge(\"error_handler\", END) # End if error is handled\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "# Compile the graph\n",
        "strategic_pipeline = build_strategic_pipeline_graph()\n",
        "\n",
        "\n",
        "# --- 6. Main Execution Function ---\n",
        "\n",
        "async def run_strategic_analysis(user_input_json: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Main function to run the strategic analysis pipeline.\n",
        "    Takes user input as a dictionary, validates it, and runs the LangGraph.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        user_input = FrontendInput(**user_input_json)\n",
        "    except ValidationError as e:\n",
        "        print(f\"Input validation error: {e.errors()}\")\n",
        "        return {\n",
        "            \"error\": \"Invalid input format\",\n",
        "            \"details\": e.errors()\n",
        "        }\n",
        "\n",
        "    # Set the current company context in the DataManager BEFORE running the pipeline\n",
        "    data_manager.set_current_company(user_input.current_company_id)\n",
        "\n",
        "    print(f\"\\n--- Starting Analysis for Objective: {user_input.strategic_objective.value} (Company: {user_input.current_company_id}) ---\")\n",
        "\n",
        "    initial_state = AgentState(\n",
        "        messages=[HumanMessage(content=f\"Analyze portfolio for: {user_input.strategic_objective.value} with filters: {user_input.filters.model_dump_json(exclude_none=True)}\")],\n",
        "        user_input=user_input,\n",
        "        tool_output=None,\n",
        "        strategic_output=None,\n",
        "        next_action=\"call_tool\"\n",
        "    )\n",
        "\n",
        "    final_state_output = None\n",
        "    async for state in strategic_pipeline.astream(initial_state):\n",
        "        if \"__end__\" in state:\n",
        "            final_state_output = state[\"__end__\"]\n",
        "            break\n",
        "        # Print intermediate states if debugging\n",
        "        # print(f\"Intermediate State: {state}\")\n",
        "\n",
        "    if final_state_output and \"strategic_output\" in final_state_output:\n",
        "        print(\"--- Analysis Complete ---\")\n",
        "        return final_state_output[\"strategic_output\"].model_dump()\n",
        "    else:\n",
        "        print(\"--- Analysis FAILED or No Output ---\")\n",
        "        return {\n",
        "            \"error\": \"Analysis failed to produce a valid strategic output.\",\n",
        "            \"final_state\": final_state_output # For debugging\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "1aGfYM4g7UID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNGxEKGe61uV",
        "outputId": "1c1bd805-d9ce-4b6b-fdcf-b3e30a6ce432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=b76d586488421644ea2feffb6eee4b2457e98ce348e35bdcfe5b7f323435e590\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    async def main():\n",
        "        # Example 1: Pruning the Portfolio for InnovatePharma Inc.\n",
        "        pruning_input_json = {\n",
        "            \"filters\": {\n",
        "                \"for_sale\": \"all\",\n",
        "                \"asset_type\": \"Molecule\",\n",
        "                \"asset_phenotype\": {\n",
        "                    \"stages_of_development\": [\"Phase 2\", \"Phase 3\", \"Marketed\"],\n",
        "                    \"therapeutic_area\": [\"Metabolic\", \"Oncology\", \"Dermatology\"]\n",
        "                }\n",
        "            },\n",
        "            \"strategic_objective\": \"Pruning the Portfolio\",\n",
        "            # CORRECTED: Using the correct company ID from your JSON\n",
        "            \"current_company_id\": \"YOUR_COMPANY_001\"\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Running Pruning Analysis ---\")\n",
        "        try:\n",
        "            pruning_output = await run_strategic_analysis(pruning_input_json)\n",
        "            print(\"\\nPruning Output:\")\n",
        "            # Use json.dumps to pretty print the output\n",
        "            print(json.dumps(pruning_output, indent=2))\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during strategic analysis: {e}\")\n",
        "\n",
        "    await main() # Directly await the main function in an interactive environment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ0mhJFL_6cS",
        "outputId": "c051beb3-915e-457b-d954-4d56da638036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Pruning Analysis ---\n",
            "\n",
            "--- Starting Analysis for Objective: Pruning the Portfolio (Company: YOUR_COMPANY_001) ---\n",
            "An error occurred during strategic analysis: 'StructuredTool' object has no attribute '__name__'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # import asyncio\n",
        "\n",
        "    async def main():\n",
        "        # Example 1: Pruning the Portfolio for InnovatePharma Inc.\n",
        "        pruning_input_json = {\n",
        "            \"filters\": {\n",
        "                \"for_sale\": \"all\",\n",
        "                \"asset_type\": \"Molecule\",\n",
        "                \"asset_phenotype\": {\n",
        "                    \"stages_of_development\": [\"Phase 2\", \"Phase 3\", \"Marketed\"],\n",
        "                    \"therapeutic_area\": [\"Metabolic\", \"Oncology\", \"Dermatology\"]\n",
        "                }\n",
        "            },\n",
        "            \"strategic_objective\": \"Pruning the Portfolio\",\n",
        "            \"current_company_id\": \"YOUR_COMPANY_001\"\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Running Pruning Analysis ---\")\n",
        "        pruning_output = await run_strategic_analysis(pruning_input_json)\n",
        "        print(\"\\nPruning Output:\")\n",
        "        print(json.dumps(pruning_output, indent=2))\n",
        "\n",
        "        # # Example 2: Diversification for InnovatePharma Inc.\n",
        "        # diversification_input_json = {\n",
        "        #     \"filters\": {\n",
        "        #         \"for_sale\": \"for_purchase\",\n",
        "        #         \"asset_type\": \"Molecule\",\n",
        "        #         \"asset_phenotype\": {\n",
        "        #             \"therapeutic_area\": [\"Rare Diseases\", \"Neuroscience\"],\n",
        "        #             \"modality\": [\"Gene Therapy\", \"Cell Therapy\"]\n",
        "        #         },\n",
        "        #         \"search_query\": \"novel gene therapy for neurodegenerative disorders with high unmet need\"\n",
        "        #     },\n",
        "        #     \"strategic_objective\": \"Diversification\",\n",
        "        #     \"current_company_id\": \"YOUR_COMPANY_001\"\n",
        "        # }\n",
        "        # print(\"\\n--- Running Diversification Analysis ---\")\n",
        "        # diversification_output = await run_strategic_analysis(diversification_input_json)\n",
        "        # print(\"\\nDiversification Output:\")\n",
        "        # print(json.dumps(diversification_output, indent=2))\n",
        "\n",
        "        # # Example 3: Filling the Pipeline (looking for Preclinical Oncology assets) for InnovatePharma Inc.\n",
        "        # filling_input_json = {\n",
        "        #     \"filters\": {\n",
        "        #         \"for_sale\": \"for_purchase\",\n",
        "        #         \"asset_type\": \"Molecule\",\n",
        "        #         \"asset_phenotype\": {\n",
        "        #             \"stages_of_development\": [\"Preclinical\", \"Phase 1\"],\n",
        "        #             \"therapeutic_area\": [\"Oncology\"]\n",
        "        #         }\n",
        "        #     },\n",
        "        #     \"strategic_objective\": \"Filling the Pipeline\",\n",
        "        #     \"current_company_id\": \"YOUR_COMPANY_001\"\n",
        "        # }\n",
        "        # print(\"\\n--- Running Filling Pipeline Analysis ---\")\n",
        "        # filling_output = await run_strategic_analysis(filling_input_json)\n",
        "        # print(\"\\nFilling Pipeline Output:\")\n",
        "        # print(json.dumps(filling_output, indent=2))\n",
        "\n",
        "        # # Example 4: Augmenting the Pipeline (looking for new indications for existing cardio assets) for InnovatePharma Inc.\n",
        "        # augmenting_input_json = {\n",
        "        #     \"filters\": {\n",
        "        #         \"asset_type\": \"Molecule\",\n",
        "        #         \"asset_phenotype\": {\n",
        "        #             \"therapeutic_area\": [\"Cardiovascular\"],\n",
        "        #             \"stages_of_development\": [\"Phase 2\", \"Phase 3\", \"Marketed\"]\n",
        "        #         },\n",
        "        #         \"search_query\": \"new applications for enzyme replacement therapy in heart conditions\"\n",
        "        #     },\n",
        "        #     \"strategic_objective\": \"Augmenting the Pipeline / New Indication Space\",\n",
        "        #     \"current_company_id\": \"YOUR_COMPANY_001\"\n",
        "        # }\n",
        "        # print(\"\\n--- Running Augmenting Pipeline Analysis ---\")\n",
        "        # augmenting_output = await run_strategic_analysis(augmenting_input_json)\n",
        "        # print(\"\\nAugmenting Pipeline Output:\")\n",
        "        # print(json.dumps(augmenting_output, indent=2))\n",
        "\n",
        "        # # Example 5: Custom Query\n",
        "        # custom_query_input_json = {\n",
        "        #     \"filters\": {\n",
        "        #         \"search_query\": \"What are the key trends in metabolic disease drug development for small molecules?\"\n",
        "        #     },\n",
        "        #     \"strategic_objective\": \"Custom Query\",\n",
        "        #     \"custom_query_text\": \"Provide insights on the key trends in metabolic disease drug development, focusing on small molecules and potential future targets.\",\n",
        "        #     \"current_company_id\": \"YOUR_COMPANY_001\"\n",
        "        # }\n",
        "        # print(\"\\n--- Running Custom Query Analysis ---\")\n",
        "        # custom_query_output = await run_strategic_analysis(custom_query_input_json)\n",
        "        # print(\"\\nCustom Query Output:\")\n",
        "        # print(json.dumps(custom_query_output, indent=2))\n",
        "\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "O75PqoH86ftb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "798a03ab-c0f8-44b7-d8fc-3d46460a7d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-1242772841.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# print(json.dumps(custom_query_output, indent=2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional, Literal, TypedDict\n",
        "from pydantic import BaseModel, Field, conint, confloat, ValidationError\n",
        "from enum import Enum\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Union\n",
        "\n",
        "# LangChain specific imports\n",
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.tools import tool, StructuredTool # Import StructuredTool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_core.documents import Document # Moved here for consistency\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode # Import ToolNode from langgraph.prebuilt\n",
        "\n",
        "# Ensure your GOOGLE_API_KEY is correctly set.\n",
        "GOOGLE_API_KEY = \"AIzaSyC4q-ry8oPTjBHDP1suYrtB2PX52MXREwg\"\n",
        "\n",
        "LANGCHAIN_API_KEY = \"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\" # Optional, for Langsmith tracing\n",
        "LANGCHAIN_TRACING_V2 = \"true\" # Enable Langsmith for tracing!\n",
        "LANGCHAIN_PROJECT = \"Strategic Portfolio AI\"\n",
        "\n",
        "# --- 1. Enums and Pydantic Models for Data and Input/Output ---\n",
        "\n",
        "# ENUMS for fixed, categorical choices (UI-driven)\n",
        "class AssetType(str, Enum):\n",
        "    MOLECULE = \"Molecule\"\n",
        "    PLATFORM = \"Platform\"\n",
        "    DEVICE = \"Device\"\n",
        "\n",
        "class StrategicObjectiveEnum(str, Enum):\n",
        "    PRUNING = \"Pruning the Portfolio\"\n",
        "    DIVERSIFICATION = \"Diversification\"\n",
        "    FILLING_PIPELINE = \"Filling the Pipeline\"\n",
        "    AUGMENTING = \"Augmenting the Pipeline / New Indication Space\" # Changed from AUGMENTING_PIPELINE to AUGMENTING for brevity\n",
        "    CUSTOM_QUERY = \"Custom Query\"\n",
        "\n",
        "# Pydantic Models for Data (flexible string types for dynamic fields)\n",
        "class Molecule(BaseModel):\n",
        "    id: str\n",
        "    name: str\n",
        "    ndc_code: Optional[str] = None\n",
        "    asset_type: AssetType\n",
        "    development_stage: str\n",
        "    therapeutic_area: str\n",
        "    indication: str\n",
        "    mechanism_of_action: str\n",
        "    route_of_administration: str\n",
        "    modality: str\n",
        "    patent_expiry_year: int\n",
        "    current_roi: Optional[float] = None\n",
        "    projected_peak_sales_M: Optional[confloat(ge=0)] = None\n",
        "    internal_risk_score: confloat(ge=0, le=1)\n",
        "    efficacy_profile: str\n",
        "    safety_profile: str\n",
        "    is_for_sale: bool = False\n",
        "    company_id: str\n",
        "    company_name: str\n",
        "\n",
        "class Company(BaseModel):\n",
        "    id: str\n",
        "    name: str\n",
        "    type: str\n",
        "    development_stage: str\n",
        "    headquarters: str\n",
        "    financial_status: str\n",
        "    partner_status: Optional[str] = None\n",
        "    territory: str\n",
        "\n",
        "class ExternalTrialData(BaseModel):\n",
        "    trial_id: str\n",
        "    molecule_id: str\n",
        "    indication: str\n",
        "    development_stage: str\n",
        "    results_summary: str\n",
        "    safety_profile: str\n",
        "    efficacy_profile: str\n",
        "    target_biology: str\n",
        "    pathway_overlap: List[str] = Field(default_factory=list)\n",
        "    patient_population_characteristics: str\n",
        "\n",
        "class MarketBenchmark(BaseModel):\n",
        "    therapeutic_area: str\n",
        "    development_stage: str\n",
        "    avg_roi: float\n",
        "    avg_peak_sales_M: confloat(ge=0)\n",
        "    avg_time_in_stage_months: conint(gt=0)\n",
        "    success_rate: confloat(ge=0, le=1)\n",
        "\n",
        "# Frontend Input Model (Adapted to new data types for filters)\n",
        "class AssetPhenotypeFilters(BaseModel):\n",
        "    stages_of_development: Optional[List[str]] = None\n",
        "    therapeutic_area: Optional[List[str]] = None\n",
        "    indication: Optional[List[str]] = None\n",
        "    mechanism_of_action: Optional[List[str]] = None\n",
        "    route_of_administration: Optional[List[str]] = None\n",
        "    modality: Optional[List[str]] = None\n",
        "    patent_expiry_yr: Optional[conint(gt=1900, le=2100)] = None\n",
        "\n",
        "class CompanyDetailsFilters(BaseModel):\n",
        "    company_type: Optional[List[str]] = None\n",
        "    development_stage: Optional[List[str]] = None\n",
        "    headquarters: Optional[List[str]] = None\n",
        "    financial_status: Optional[List[str]] = None\n",
        "    partner_status: Optional[List[str]] = None\n",
        "    territory: Optional[List[str]] = None\n",
        "\n",
        "class PeakSalesFilters(BaseModel):\n",
        "    one_yr_sales_potential_M: Optional[confloat(ge=0)] = None\n",
        "    five_yr_sales_potential_M: Optional[confloat(ge=0)] = None\n",
        "    peak_sales_M: Optional[confloat(ge=0)] = None\n",
        "\n",
        "class FrontendFilters(BaseModel):\n",
        "    for_sale: Literal[\"all\", \"for_sale\", \"for_purchase\", \"sold\"] = \"all\"\n",
        "    asset_type: Optional[AssetType] = None\n",
        "    deal_value_min: Optional[confloat(ge=0)] = None\n",
        "    deal_value_max: Optional[confloat(ge=0)] = None\n",
        "    asset_phenotype: Optional[AssetPhenotypeFilters] = Field(default_factory=AssetPhenotypeFilters)\n",
        "    company_details: Optional[CompanyDetailsFilters] = Field(default_factory=CompanyDetailsFilters)\n",
        "    peak_sales: Optional[PeakSalesFilters] = Field(default_factory=PeakSalesFilters)\n",
        "    search_query: Optional[str] = None\n",
        "\n",
        "class FrontendInput(BaseModel):\n",
        "    filters: FrontendFilters = Field(default_factory=FrontendFilters)\n",
        "    strategic_objective: StrategicObjectiveEnum\n",
        "    custom_query_text: Optional[str] = None\n",
        "    current_company_id: str # This ID comes with the user input\n",
        "\n",
        "\n",
        "# Output Structures for Recommendations\n",
        "class PruningRecommendation(BaseModel):\n",
        "    action_type: Literal[\"Deprioritize Asset\"]\n",
        "    molecule_name: str\n",
        "    molecule_id: str\n",
        "    justification: str\n",
        "    reason_criteria: List[str]\n",
        "    risk_score: confloat(ge=0, le=1)\n",
        "    opportunity_cost_estimate: str\n",
        "    impact_on_portfolio: Dict[str, Any]\n",
        "\n",
        "class DiversificationRecommendation(BaseModel):\n",
        "    action_type: Literal[\"Acquire Asset\", \"Invest in Research Area\"]\n",
        "    molecule_name: Optional[str] = None\n",
        "    molecule_id: Optional[str] = None\n",
        "    reason_for_diversification: str\n",
        "    strategic_fit_score: confloat(ge=0, le=1)\n",
        "    target_disease_area: str\n",
        "    proposed_moa: str\n",
        "    proposed_modality: str\n",
        "\n",
        "class FillingPipelineRecommendation(BaseModel):\n",
        "    action_type: Literal[\"Acquire Asset\", \"Initiate Internal Project\"]\n",
        "    molecule_name: str\n",
        "    molecule_id: str\n",
        "    reason_for_suggestion: str\n",
        "    suggested_role: str\n",
        "    development_stage_fit: str\n",
        "\n",
        "class AugmentingPipelineRecommendation(BaseModel):\n",
        "    action_type: Literal[\"Initiate New Indication Trial\"]\n",
        "    molecule_name: str\n",
        "    molecule_id: str\n",
        "    new_indication: str\n",
        "    justification: str\n",
        "    evidence_strength: Literal[\"Low\", \"Moderate\", \"High\", \"Very High\"]\n",
        "    market_potential_estimate: str\n",
        "\n",
        "class StrategicOutput(BaseModel):\n",
        "    strategic_objective_addressed: StrategicObjectiveEnum\n",
        "    # *** CRITICAL CHANGE HERE ***\n",
        "    recommendations: List[\n",
        "        Union[\n",
        "            PruningRecommendation,\n",
        "            DiversificationRecommendation,\n",
        "            FillingPipelineRecommendation,\n",
        "            AugmentingPipelineRecommendation\n",
        "        ]\n",
        "    ] = Field(default_factory=list) # Added default_factory for robustness\n",
        "    strategic_summary: str\n",
        "    overall_impact_on_portfolio: Dict[str, Any]\n",
        "    recommended_portfolio_adjustments: Dict[str, Any]\n",
        "    suggested_ideal_portfolio_characteristics: Dict[str, Any]\n",
        "\n",
        "# Generate JSON schemas for the output models\n",
        "strategic_output_schema = StrategicOutput.model_json_schema()\n",
        "pruning_recommendation_schema = PruningRecommendation.model_json_schema()\n",
        "\n",
        "# --- Initialize Global LLM and Embeddings ---\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2, google_api_key=GOOGLE_API_KEY)\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "\n",
        "# --- 2. Data Management Layer ---\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self, data_file_path: str, embeddings_model: GoogleGenerativeAIEmbeddings):\n",
        "        self.data_file_path = data_file_path\n",
        "        self.embeddings = embeddings_model\n",
        "        self._load_all_data() # Load all data once at initialization\n",
        "\n",
        "        # Initialize these as None; they will be populated when set_current_company is called\n",
        "        self.internal_portfolio_vs: Optional[Chroma] = None\n",
        "        self.competitive_landscape_vs: Optional[Chroma] = None\n",
        "        self.external_molecule_database_vs: Optional[Chroma] = None\n",
        "        self.external_trial_data_vs: Optional[Chroma] = None\n",
        "        self.literature_vs: Optional[Chroma] = None\n",
        "\n",
        "        self._initialize_static_vector_stores()\n",
        "\n",
        "\n",
        "    def _initialize_static_vector_stores(self):\n",
        "        \"\"\"Initializes vector stores that do not depend on `your_company_id`.\"\"\"\n",
        "        # Initialize ChromaDB vector stores that don't depend on self.your_company_id\n",
        "        # The external_molecule_database_vs and external_trial_data_vs can be initialized with all data\n",
        "        self.external_molecule_database_vs = Chroma.from_documents(\n",
        "            documents=[doc for m in self.all_molecules.values() for doc in self._molecule_to_documents(m)] if self.all_molecules else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"external_molecule_database_v3\"\n",
        "        )\n",
        "        self.external_trial_data_vs = Chroma.from_documents(\n",
        "            documents=[doc for t in self.external_trial_data.values() for doc in self._trial_to_documents(t)] if self.external_trial_data else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"external_trial_data_v3\"\n",
        "        )\n",
        "        self.literature_vs = Chroma.from_texts(\n",
        "            texts=[s[\"text\"] for s in self.literature_snippets] if self.literature_snippets else [],\n",
        "            metadatas=[{\"source\": s[\"source\"]} for s in self.literature_snippets] if self.literature_snippets else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"literature_snippets_v3\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def _load_all_data(self):\n",
        "        \"\"\"Loads and parses all data from the single JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(self.data_file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            self.all_companies: Dict[str, Company] = {c[\"id\"]: Company(**c) for c in data.get(\"companies\", [])}\n",
        "            self.all_molecules: Dict[str, Molecule] = {m[\"id\"]: Molecule(**m) for m in data.get(\"molecules\", [])}\n",
        "            self.external_trial_data: Dict[str, ExternalTrialData] = {t[\"trial_id\"]: ExternalTrialData(**t) for t in data.get(\"external_trial_data\", [])}\n",
        "            self.market_benchmarks: List[MarketBenchmark] = [MarketBenchmark(**bm) for bm in data.get(\"market_benchmarks\", [])]\n",
        "            self.literature_snippets: List[Dict[str,str]] = data.get(\"literature_snippets\", [])\n",
        "\n",
        "            print(f\"Loaded {len(self.all_companies)} companies.\")\n",
        "            print(f\"Loaded {len(self.all_molecules)} molecules.\")\n",
        "            print(f\"Loaded {len(self.external_trial_data)} external trial data entries.\")\n",
        "            print(f\"Loaded {len(self.market_benchmarks)} market benchmarks.\")\n",
        "            print(f\"Loaded {len(self.literature_snippets)} literature snippets.\")\n",
        "\n",
        "            # These will be set dynamically based on user input\n",
        "            self.your_company_id: Optional[str] = None\n",
        "            self.your_company_profile: Optional[Company] = None\n",
        "            self.internal_molecules: Dict[str, Molecule] = {}\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Data file not found at: {self.data_file_path}\")\n",
        "        except ValidationError as e:\n",
        "            print(f\"Pydantic validation error when loading data: {e.errors()}\")\n",
        "            raise\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(f\"Error decoding JSON from {self.data_file_path}. Ensure it's valid JSON.\")\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Missing expected key in JSON data: {e}. Check your JSON structure.\")\n",
        "\n",
        "    def set_current_company(self, company_id: str):\n",
        "        \"\"\"Sets the current company for the session and initializes company-specific vector stores.\"\"\"\n",
        "        self.your_company_id = company_id\n",
        "        self.your_company_profile = self.all_companies.get(company_id)\n",
        "        self.internal_molecules = {\n",
        "            mol.id: mol for mol in self.all_molecules.values() if mol.company_id == company_id\n",
        "        }\n",
        "        if not self.your_company_profile:\n",
        "            print(f\"Warning: Current company ID '{company_id}' not found in loaded data. Internal portfolio will be empty.\")\n",
        "\n",
        "        # Initialize internal_portfolio_vs and competitive_landscape_vs here\n",
        "        self.internal_portfolio_vs = Chroma.from_documents(\n",
        "            documents=[doc for m in self.internal_molecules.values() for doc in self._molecule_to_documents(m)] if self.internal_molecules else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"internal_portfolio_v3\"\n",
        "        )\n",
        "        self.competitive_landscape_vs = Chroma.from_documents(\n",
        "            documents=[doc for m in self.all_molecules.values() if m.company_id != self.your_company_id for doc in self._molecule_to_documents(m)] if self.all_molecules else [],\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"competitive_landscape_v3\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def _molecule_to_documents(self, molecule: Molecule) -> List[Document]:\n",
        "        \"\"\"Converts a Molecule Pydantic model to LangChain Document(s).\"\"\"\n",
        "        content = (\n",
        "            f\"Molecule Name: {molecule.name}\\n\"\n",
        "            f\"ID: {molecule.id}\\n\"\n",
        "            f\"Type: {molecule.asset_type.value}\\n\"\n",
        "            f\"Stage: {molecule.development_stage}\\n\"\n",
        "            f\"Therapeutic Area: {molecule.therapeutic_area}\\n\"\n",
        "            f\"Indication: {molecule.indication}\\n\"\n",
        "            f\"Mechanism of Action: {molecule.mechanism_of_action}\\n\"\n",
        "            f\"Modality: {molecule.modality}\\n\"\n",
        "            f\"Patent Expiry: {molecule.patent_expiry_year}\\n\"\n",
        "            f\"Projected Peak Sales: ${molecule.projected_peak_sales_M or 'N/A'}M\\n\"\n",
        "            f\"Internal Risk Score: {molecule.internal_risk_score}\\n\"\n",
        "            f\"Efficacy: {molecule.efficacy_profile}\\n\"\n",
        "            f\"Safety: {molecule.safety_profile}\\n\"\n",
        "            f\"For Sale: {molecule.is_for_sale}\\n\"\n",
        "            f\"Company: {molecule.company_name} ({molecule.company_id})\"\n",
        "        )\n",
        "        metadata = molecule.model_dump()\n",
        "        metadata[\"asset_type\"] = metadata[\"asset_type\"].value # Ensure enum is string\n",
        "        return [Document(page_content=content, metadata=metadata)]\n",
        "\n",
        "    def _trial_to_documents(self, trial: ExternalTrialData) -> List[Document]:\n",
        "        \"\"\"Converts an ExternalTrialData Pydantic model to LangChain Document(s).\"\"\"\n",
        "        content = (\n",
        "            f\"Trial ID: {trial.trial_id}\\n\"\n",
        "            f\"Molecule ID: {trial.molecule_id}\\n\"\n",
        "            f\"Indication: {trial.indication}\\n\"\n",
        "            f\"Stage: {trial.development_stage}\\n\"\n",
        "            f\"Results: {trial.results_summary}\\n\"\n",
        "            f\"Safety: {trial.safety_profile}\\n\"\n",
        "            f\"Efficacy: {trial.efficacy_profile}\\n\"\n",
        "            f\"Target Biology: {trial.target_biology}\\n\"\n",
        "            f\"Pathway Overlap: {', '.join(trial.pathway_overlap)}\\n\" # FIX: Join list into string\n",
        "            f\"Patient Population: {trial.patient_population_characteristics}\"\n",
        "        )\n",
        "        metadata = trial.model_dump()\n",
        "        # FIX: Convert pathway_overlap list to a comma-separated string for metadata\n",
        "        metadata[\"pathway_overlap\"] = \", \".join(trial.pathway_overlap)\n",
        "        return [Document(page_content=content, metadata=metadata)]\n",
        "\n",
        "    def get_molecule_by_id(self, molecule_id: str) -> Optional[Molecule]:\n",
        "        return self.all_molecules.get(molecule_id)\n",
        "\n",
        "    def get_company_by_id(self, company_id: str) -> Optional[Company]:\n",
        "        return self.all_companies.get(company_id)\n",
        "\n",
        "    def get_market_benchmark(self, ta: str, ds: str) -> Optional[MarketBenchmark]:\n",
        "        for bm in self.market_benchmarks:\n",
        "            if bm.therapeutic_area.lower() == ta.lower() and bm.development_stage.lower() == ds.lower():\n",
        "                return bm\n",
        "        return None\n",
        "\n",
        "    def get_internal_portfolio(self) -> List[Molecule]:\n",
        "        return list(self.internal_molecules.values())\n",
        "\n",
        "    def get_all_molecules(self) -> List[Molecule]:\n",
        "        return list(self.all_molecules.values())\n",
        "\n",
        "    def get_filtered_molecules(self, molecules: List[Molecule], filters: FrontendFilters) -> List[Molecule]:\n",
        "        \"\"\"\n",
        "        Splitting this function into several smaller parts:\n",
        "        - _apply_asset_filters\n",
        "        - _apply_deal_value_filters\n",
        "        - _apply_phenotype_filters\n",
        "        - _apply_company_filters\n",
        "        \"\"\"\n",
        "        filtered_list = []\n",
        "        for mol in molecules:\n",
        "            # Apply 'for_sale' filter\n",
        "            if filters.for_sale == \"for_sale\" and not mol.is_for_sale:\n",
        "                continue\n",
        "            if filters.for_sale == \"for_purchase\" and (not mol.is_for_sale or mol.company_id == self.your_company_id):\n",
        "                continue\n",
        "            # \"sold\" is a historical state, not a current filter on available assets\n",
        "\n",
        "            # Apply asset_type filter\n",
        "            if filters.asset_type and mol.asset_type != filters.asset_type:\n",
        "                continue\n",
        "\n",
        "            # Apply deal value filters (based on projected peak sales)\n",
        "            if filters.deal_value_min is not None and (mol.projected_peak_sales_M is None or mol.projected_peak_sales_M < filters.deal_value_min):\n",
        "                continue\n",
        "            if filters.deal_value_max is not None and (mol.projected_peak_sales_M is None or mol.projected_peak_sales_M > filters.deal_value_max):\n",
        "                continue\n",
        "            if filters.peak_sales:\n",
        "                if filters.peak_sales.peak_sales_M is not None and (mol.projected_peak_sales_M is None or mol.projected_peak_sales_M < filters.peak_sales.peak_sales_M):\n",
        "                    continue\n",
        "                # 1yr/5yr sales are not in Molecule model, skip for now.\n",
        "\n",
        "            # Apply AssetPhenotypeFilters\n",
        "            if filters.asset_phenotype:\n",
        "                if filters.asset_phenotype.stages_of_development and mol.development_stage not in filters.asset_phenotype.stages_of_development:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.therapeutic_area and mol.therapeutic_area not in filters.asset_phenotype.therapeutic_area:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.indication and mol.indication not in filters.asset_phenotype.indication:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.mechanism_of_action and mol.mechanism_of_action not in filters.asset_phenotype.mechanism_of_action:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.route_of_administration and mol.route_of_administration not in filters.asset_phenotype.route_of_administration:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.modality and mol.modality not in filters.asset_phenotype.modality:\n",
        "                    continue\n",
        "                if filters.asset_phenotype.patent_expiry_yr is not None and mol.patent_expiry_year < filters.asset_phenotype.patent_expiry_yr:\n",
        "                    continue\n",
        "\n",
        "            # Apply CompanyDetailsFilters (requires looking up company info)\n",
        "            if filters.company_details:\n",
        "                company_of_mol = self.get_company_by_id(mol.company_id)\n",
        "                if company_of_mol:\n",
        "                    if filters.company_details.company_type and company_of_mol.type not in filters.company_details.company_type:\n",
        "                        continue\n",
        "                    if filters.company_details.development_stage and company_of_mol.development_stage not in filters.company_details.development_stage:\n",
        "                        continue\n",
        "                    if filters.company_details.headquarters and company_of_mol.headquarters not in filters.company_details.headquarters:\n",
        "                        continue\n",
        "                    if filters.company_details.financial_status and company_of_mol.financial_status not in filters.company_details.financial_status:\n",
        "                        continue\n",
        "                    if filters.company_details.partner_status and company_of_mol.partner_status not in filters.company_details.partner_status:\n",
        "                        continue\n",
        "                    if filters.company_details.territory and company_of_mol.territory not in filters.company_details.territory:\n",
        "                        continue\n",
        "                else: # If company data for molecule is missing or doesn't exist, it doesn't match\n",
        "                    continue\n",
        "\n",
        "            filtered_list.append(mol)\n",
        "        return filtered_list\n",
        "\n",
        "# Instantiate DataManager (assuming 'company.json' exists in the same directory)\n",
        "data_manager = DataManager('/content/company.json', embeddings)\n",
        "\n",
        "\n",
        "# --- 3. Tool Definitions ---\n",
        "\n",
        "@tool\n",
        "def get_company_details(company_id: str) -> Company:\n",
        "    \"\"\"\n",
        "    Retrieves detailed information about a specific company by its ID.\n",
        "    Useful for understanding the profile of a competitor or partner.\n",
        "    \"\"\"\n",
        "    company = data_manager.get_company_by_id(company_id)\n",
        "    if not company:\n",
        "        raise ValueError(f\"Company with ID {company_id} not found.\")\n",
        "    return company\n",
        "\n",
        "@tool\n",
        "def get_molecule_details(molecule_id: str) -> Molecule:\n",
        "    \"\"\"\n",
        "    Retrieves detailed information about a specific molecule by its ID.\n",
        "    Useful for understanding the characteristics of an asset in the portfolio or competitive landscape.\n",
        "    \"\"\"\n",
        "    molecule = data_manager.get_molecule_by_id(molecule_id)\n",
        "    if not molecule:\n",
        "        raise ValueError(f\"Molecule with ID {molecule_id} not found.\")\n",
        "    return molecule\n",
        "\n",
        "@tool\n",
        "def search_internal_portfolio(query: Optional[str] = None, filters: Optional[FrontendFilters] = None) -> List[Molecule]:\n",
        "    \"\"\"\n",
        "    Searches the current company's internal molecule portfolio using a natural language query and/or structured filters.\n",
        "    Provides detailed information about internal assets that match the criteria.\n",
        "    If no query or filters are provided, returns all internal molecules.\n",
        "    \"\"\"\n",
        "    if not data_manager.internal_portfolio_vs:\n",
        "        return [] # No internal portfolio set up\n",
        "\n",
        "    # If a query is provided, use semantic search\n",
        "    if query:\n",
        "        retriever = data_manager.internal_portfolio_vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "        docs = retriever.invoke(query)\n",
        "        # Convert documents back to Molecule models\n",
        "        molecules = []\n",
        "        for doc in docs:\n",
        "            mol_data = doc.metadata\n",
        "            try:\n",
        "                # Need to convert asset_type string back to AssetType Enum for Pydantic\n",
        "                mol_data['asset_type'] = AssetType(mol_data['asset_type'])\n",
        "                molecules.append(Molecule(**mol_data))\n",
        "            except ValidationError as e:\n",
        "                print(f\"Validation error converting document metadata to Molecule: {e}\")\n",
        "                continue\n",
        "        # Apply structured filters to the semantically retrieved results\n",
        "        if filters:\n",
        "            molecules = data_manager.get_filtered_molecules(molecules, filters)\n",
        "        return molecules\n",
        "    elif filters:\n",
        "        # If only filters are provided, get all internal molecules and then filter\n",
        "        all_internal_mols = data_manager.get_internal_portfolio()\n",
        "        return data_manager.get_filtered_molecules(all_internal_mols, filters)\n",
        "    else:\n",
        "        # If neither query nor filters, return all internal molecules\n",
        "        return data_manager.get_internal_portfolio()\n",
        "\n",
        "@tool\n",
        "def search_competitive_landscape(query: Optional[str] = None, filters: Optional[FrontendFilters] = None) -> List[Molecule]:\n",
        "    \"\"\"\n",
        "    Searches the competitive landscape (molecules owned by other companies) using a natural language query and/or structured filters.\n",
        "    Provides detailed information about competitor assets that match the criteria.\n",
        "    If no query or filters are provided, returns all competitive molecules.\n",
        "    \"\"\"\n",
        "    if not data_manager.competitive_landscape_vs:\n",
        "        return [] # No competitive landscape set up (shouldn't happen if data loaded)\n",
        "\n",
        "    if query:\n",
        "        retriever = data_manager.competitive_landscape_vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "        docs = retriever.invoke(query)\n",
        "        molecules = []\n",
        "        for doc in docs:\n",
        "            mol_data = doc.metadata\n",
        "            try:\n",
        "                mol_data['asset_type'] = AssetType(mol_data['asset_type'])\n",
        "                molecules.append(Molecule(**mol_data))\n",
        "            except ValidationError as e:\n",
        "                print(f\"Validation error converting document metadata to Molecule: {e}\")\n",
        "                continue\n",
        "        if filters:\n",
        "            molecules = data_manager.get_filtered_molecules(molecules, filters)\n",
        "        return molecules\n",
        "    elif filters:\n",
        "        all_competitive_mols = [\n",
        "            mol for mol in data_manager.get_all_molecules()\n",
        "            if mol.company_id != data_manager.your_company_id\n",
        "        ]\n",
        "        return data_manager.get_filtered_molecules(all_competitive_mols, filters)\n",
        "    else:\n",
        "        return [mol for mol in data_manager.get_all_molecules() if mol.company_id != data_manager.your_company_id]\n",
        "\n",
        "@tool\n",
        "def search_external_trial_data(query: str, molecule_id: Optional[str] = None) -> List[ExternalTrialData]:\n",
        "    \"\"\"\n",
        "    Searches external clinical trial data for specific molecules or general therapeutic areas.\n",
        "    Useful for assessing efficacy, safety, and patient populations from external studies.\n",
        "    Always requires a query. Can optionally filter by molecule_id.\n",
        "    \"\"\"\n",
        "    if not data_manager.external_trial_data_vs:\n",
        "        return []\n",
        "\n",
        "    retriever = data_manager.external_trial_data_vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "    docs = retriever.invoke(query)\n",
        "    trial_data_list = []\n",
        "    for doc in docs:\n",
        "        trial_data = doc.metadata\n",
        "        # Reconstruct the pathway_overlap list from the comma-separated string\n",
        "        if \"pathway_overlap\" in trial_data and isinstance(trial_data[\"pathway_overlap\"], str):\n",
        "            trial_data[\"pathway_overlap\"] = [p.strip() for p in trial_data[\"pathway_overlap\"].split(',')]\n",
        "        try:\n",
        "            # Ensure all fields expected by ExternalTrialData are present or handled\n",
        "            trial_data_list.append(ExternalTrialData(**trial_data))\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error converting document metadata to ExternalTrialData: {e}\")\n",
        "            continue\n",
        "\n",
        "    if molecule_id:\n",
        "        return [td for td in trial_data_list if td.molecule_id == molecule_id]\n",
        "    return trial_data_list\n",
        "\n",
        "@tool\n",
        "def get_market_benchmark_data(therapeutic_area: str, development_stage: str) -> Optional[MarketBenchmark]:\n",
        "    \"\"\"\n",
        "    Retrieves market benchmark data (e.g., average ROI, peak sales, success rates) for a given therapeutic area and development stage.\n",
        "    Useful for evaluating the potential of assets against industry standards.\n",
        "    \"\"\"\n",
        "    return data_manager.get_market_benchmark(therapeutic_area, development_stage)\n",
        "\n",
        "@tool\n",
        "def search_literature(query: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Searches the scientific literature snippets for relevant information.\n",
        "    Useful for gathering background, novel insights, or supporting evidence for strategic decisions.\n",
        "    \"\"\"\n",
        "    if not data_manager.literature_vs:\n",
        "        return []\n",
        "    retriever = data_manager.literature_vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "    docs = retriever.invoke(query)\n",
        "    return [{\"text\": doc.page_content, \"source\": doc.metadata.get(\"source\", \"N/A\")} for doc in docs]\n",
        "\n",
        "# Add this tool definition within your existing tool definitions\n",
        "@tool\n",
        "def submit_strategic_analysis(analysis: StrategicOutput) -> str:\n",
        "    \"\"\"\n",
        "    Use this tool to submit the final strategic portfolio analysis and recommendations.\n",
        "    The 'analysis' argument MUST be a complete StrategicOutput object, strictly\n",
        "    adhering to its schema, including all nested recommendation types (e.g., PruningRecommendation).\n",
        "    This is the final step for outputting the strategic analysis.\n",
        "    \"\"\"\n",
        "    # This tool doesn't \"do\" anything other than act as a structured output gate.\n",
        "    # The actual data will be captured by the caller.\n",
        "    return \"Strategic analysis submitted successfully.\"\n",
        "\n",
        "\n",
        "# IMPORTANT: Collect all tools into a list to be passed to bind_tools later\n",
        "tools = [\n",
        "    get_company_details,\n",
        "    get_molecule_details,\n",
        "    search_internal_portfolio,\n",
        "    search_competitive_landscape,\n",
        "    search_external_trial_data,\n",
        "    get_market_benchmark_data,\n",
        "    submit_strategic_analysis,\n",
        "    search_literature\n",
        "]\n",
        "\n",
        "\n",
        "# --- 4. Agent State ---\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our agent.\n",
        "    - `input`: The original `FrontendInput` provided by the user.\n",
        "    - `current_company_id`: The ID of the user's company, extracted from input.\n",
        "    - `chat_history`: A list of messages between the user and the agent.\n",
        "    - `strategic_objective`: The strategic objective enum.\n",
        "    - `custom_query_text`: Free text query for \"Custom Query\" objective.\n",
        "    - `filters`: Structured filters for asset and company details.\n",
        "    - `scratchpad`: A string to store intermediate thoughts, tool outputs, or reflections for the LLM.\n",
        "    - `recommendations`: A list to store the final recommendations.\n",
        "    - `final_output`: The Pydantic model for the final structured output.\n",
        "    \"\"\"\n",
        "    input: FrontendInput\n",
        "    current_company_id: str\n",
        "    chat_history: List[BaseMessage]\n",
        "    strategic_objective: StrategicObjectiveEnum\n",
        "    custom_query_text: Optional[str]\n",
        "    filters: FrontendFilters\n",
        "    scratchpad: str\n",
        "    recommendations: List[Any]\n",
        "    final_output: Optional[StrategicOutput]\n",
        "\n",
        "\n",
        "# --- 5. Graph Nodes ---\n",
        "\n",
        "# Define the LLM with tool binding\n",
        "# You should bind the tools to the LLM that will be used for deciding which tool to call.\n",
        "# This ensures the LLM knows about the tools and their schemas.\n",
        "# This `llm_with_tools` will be used in the agent's decision-making process.\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "\n",
        "def call_tool_orchestrator(state: AgentState):\n",
        "    \"\"\"\n",
        "    This node will use the LLM to decide which tool to call based on the strategic objective\n",
        "    and the current state (filters, custom_query_text, etc.).\n",
        "    It then returns the tool call as a string that the ToolNode can execute.\n",
        "    \"\"\"\n",
        "    print(\"---CALLING TOOL ORCHESTRATOR (LLM DECIDING)---\")\n",
        "    # Define a prompt for the LLM to choose and format a tool call\n",
        "    # Now using placeholders for dynamic content\n",
        "    tool_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(\"\"\"You are an expert strategic analyst. Based on the user's strategic objective,\n",
        "        identify the most relevant tool to call and its parameters.\n",
        "        You have access to the following tools: {tool_names}.\n",
        "\n",
        "        Strategic Objective: {objective}\n",
        "        Current Company ID: {current_company_id}\n",
        "        Current Company Profile: {current_company_profile}\n",
        "        User Filters: {filters_str}\n",
        "        Custom Query Text: {custom_query_text}\n",
        "\n",
        "        Your response MUST be a single, valid tool call. Do NOT output any other text or explanation.\n",
        "        Only the tool call. For example: `tool_name(param='value', another_param=123)`.\n",
        "        If no tools are directly applicable for a direct data retrieval step,\n",
        "        consider which tool might provide initial relevant context for the objective.\n",
        "        For Pruning, prioritize `search_internal_portfolio`.\n",
        "        For Diversification/Filling/Augmenting, consider `search_competitive_landscape`, `search_external_trial_data`, or `search_literature`.\n",
        "        \"\"\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"What tool should I call to get the necessary data for the strategic objective?\")\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        # Prepare inputs for the prompt\n",
        "        prompt_inputs = {\n",
        "            \"tool_names\": ', '.join([t.name for t in tools]),\n",
        "            \"objective\": state['strategic_objective'].value,\n",
        "            \"current_company_id\": state['current_company_id'],\n",
        "            \"current_company_profile\": json.dumps(data_manager.your_company_profile.model_dump(), indent=2) if data_manager.your_company_profile else \"N/A\",\n",
        "            \"filters_str\": json.dumps(state['filters'].model_dump(), indent=2),\n",
        "            \"custom_query_text\": state['custom_query_text'] or \"N/A\"\n",
        "        }\n",
        "\n",
        "        # Render the ChatPromptTemplate into a list of messages\n",
        "        messages_to_llm = tool_prompt.invoke(prompt_inputs).messages\n",
        "\n",
        "        tool_selection = llm_with_tools.invoke(messages_to_llm)\n",
        "\n",
        "        # Extract the tool call from the AI message\n",
        "        if not tool_selection.tool_calls:\n",
        "            print(\"LLM did not return any tool calls. Returning to generate analysis directly.\")\n",
        "            return {\"scratchpad\": state['scratchpad'] + \"\\nLLM chose not to call a tool.\",\n",
        "                    \"chat_history\": state['chat_history'] + [AIMessage(content=\"LLM chose not to call a tool.\")],\n",
        "                    \"tool_to_execute\": None # Indicate no tool to execute\n",
        "                   }\n",
        "\n",
        "        tool_call = tool_selection.tool_calls[0] # Assuming one tool call is made\n",
        "        tool_name = tool_call['name']\n",
        "        tool_args = tool_call['args']\n",
        "\n",
        "        tool_call_string = f\"{tool_name}({', '.join([f'{k}={repr(v)}' for k,v in tool_args.items()])})\"\n",
        "        print(f\"LLM decided to call: {tool_call_string}\")\n",
        "\n",
        "        return {\"scratchpad\": state['scratchpad'] + f\"\\nLLM chose tool: {tool_call_string}\",\n",
        "                \"chat_history\": state['chat_history'] + [AIMessage(content=f\"LLM chose tool: {tool_call_string}\")],\n",
        "                \"tool_to_execute\": tool_call\n",
        "               }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in tool orchestration: {e}\"\n",
        "        print(error_msg)\n",
        "        return {\"scratchpad\": state['scratchpad'] + f\"\\nError: {error_msg}\",\n",
        "                \"chat_history\": state['chat_history'] + [AIMessage(content=f\"Error in tool orchestration: {error_msg}\")],\n",
        "                \"tool_to_execute\": None\n",
        "               }\n",
        "\n",
        "\n",
        "def generate_strategic_analysis(state: AgentState):\n",
        "    \"\"\"\n",
        "    Generates the strategic analysis and recommendations by instructing the LLM\n",
        "    to call the 'submit_strategic_analysis' tool with the complete StrategicOutput.\n",
        "    \"\"\"\n",
        "    print(\"---GENERATING STRATEGIC ANALYSIS---\")\n",
        "\n",
        "    # Prompt the LLM to call the 'submit_strategic_analysis' tool\n",
        "    # Include clear instructions and context\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(\"\"\"You are a Strategic Portfolio Manager AI. Your final task is to compile\n",
        "        the strategic analysis and recommendations into a structured format.\n",
        "        You have gathered all necessary information from the scratchpad.\n",
        "        Your final output MUST be a call to the `submit_strategic_analysis` tool.\n",
        "\n",
        "        The `submit_strategic_analysis` tool expects a single argument, 'analysis',\n",
        "        which must be a JSON object strictly conforming to the StrategicOutput Pydantic model.\n",
        "        Pay extreme attention to the schema of `StrategicOutput` and `PruningRecommendation`\n",
        "        (and other recommendation types if applicable).\n",
        "\n",
        "        Strategic Objective: {objective}\n",
        "        Current company ID: {current_company_id}\n",
        "        Current company profile: {current_company_profile}\n",
        "        User-provided filters: {filters_str}\n",
        "        Custom query text (if applicable): {custom_query_text}\n",
        "\n",
        "        ---StrategicOutput JSON Schema---\n",
        "        {strategic_output_schema}\n",
        "        ---End StrategicOutput JSON Schema---\n",
        "\n",
        "        ---PruningRecommendation JSON Schema---\n",
        "        {pruning_recommendation_schema}\n",
        "        ---End PruningRecommendation JSON Schema---\n",
        "\n",
        "        Based on the data and your analysis in the scratchpad, construct the full\n",
        "        `StrategicOutput` object and call the `submit_strategic_analysis` tool.\n",
        "        Ensure all required fields are populated accurately and comprehensively.\n",
        "        Focus on providing actionable recommendations for the '{objective}' objective.\n",
        "\n",
        "        Example of a partial call structure (DO NOT use verbatim, populate with actual data):\n",
        "        submit_strategic_analysis(analysis={{\"strategic_objective_addressed\": \"Pruning the Portfolio\", \"recommendations\": [{{\"action_type\": \"Deprioritize Asset\", \"molecule_name\": \"...\", \"molecule_id\": \"...\", \"justification\": \"...\", \"reason_criteria\": [\"...\", \"...\"], \"risk_score\": 0.X, \"opportunity_cost_estimate\": \"...\", \"impact_on_portfolio\": {{\"...\": \"...\"}}}}], \"strategic_summary\": \"...\", \"overall_impact_on_portfolio\": {{\"...\": \"...\"}}, \"recommended_portfolio_adjustments\": {{\"...\": \"...\"}}, \"suggested_ideal_portfolio_characteristics\": {{\"...\": \"...\"}}}})\n",
        "\n",
        "        Remember to provide concrete recommendations for pruning assets from YOUR_COMPANY_001.\n",
        "        Identify specific molecules to deprioritize, justify why, and detail the expected impact.\n",
        "        \"\"\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"Here is the gathered information and my thoughts so far:\\n\\n{scratchpad}\\n\\nNow, call the `submit_strategic_analysis` tool with the complete analysis.\")\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        prompt_inputs = {\n",
        "            \"objective\": state['strategic_objective'].value,\n",
        "            \"current_company_id\": state['current_company_id'],\n",
        "            \"current_company_profile\": json.dumps(data_manager.your_company_profile.model_dump(), indent=2) if data_manager.your_company_profile else \"N/A\",\n",
        "            \"filters_str\": json.dumps(state['filters'].model_dump(), indent=2),\n",
        "            \"custom_query_text\": state['custom_query_text'] or \"N/A\",\n",
        "            \"scratchpad\": state['scratchpad'],\n",
        "            \"strategic_output_schema\": json.dumps(strategic_output_schema, indent=2),\n",
        "            \"pruning_recommendation_schema\": json.dumps(pruning_recommendation_schema, indent=2)\n",
        "        }\n",
        "\n",
        "        # Bind all tools so the LLM knows about `submit_strategic_analysis`\n",
        "        llm_with_all_tools = llm.bind_tools(tools)\n",
        "\n",
        "        # Invoke the chain. The LLM will respond with a tool call.\n",
        "        ai_message = llm_with_all_tools.invoke(prompt_template.invoke(prompt_inputs).messages)\n",
        "\n",
        "        if not ai_message.tool_calls:\n",
        "            raise ValueError(\"LLM did not make a tool call for strategic analysis. This is unexpected.\")\n",
        "\n",
        "        # Find the submit_strategic_analysis tool call\n",
        "        submit_call = next((tc for tc in ai_message.tool_calls if tc['name'] == 'submit_strategic_analysis'), None)\n",
        "\n",
        "        if not submit_call:\n",
        "            raise ValueError(\"LLM did not call the 'submit_strategic_analysis' tool.\")\n",
        "\n",
        "        # The 'analysis' argument of the tool call should be the StrategicOutput object\n",
        "        # LangChain's tool binding ensures these arguments are already parsed into Python types\n",
        "        # if the LLM successfully generated valid JSON arguments.\n",
        "        strategic_analysis_data = submit_call['args']['analysis']\n",
        "\n",
        "        # Manually validate if necessary, though the tool binding should handle basic type checks\n",
        "        validated_output = StrategicOutput(**strategic_analysis_data)\n",
        "\n",
        "        return {\"final_output\": validated_output, \"chat_history\": state['chat_history'] + [AIMessage(content=f\"Final Analysis Submitted Successfully.\")]}\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error generating strategic analysis or processing tool call: {e}\\nRaw LLM message: {ai_message if 'ai_message' in locals() else 'N/A'}\"\n",
        "        print(error_msg)\n",
        "        return {\"scratchpad\": state['scratchpad'] + f\"\\nError: {error_msg}\", \"final_output\": None, \"chat_history\": state['chat_history'] + [AIMessage(content=f\"Error: {error_msg}\")]}\n",
        "\n",
        "# --- 6. LangGraph Setup ---\n",
        "\n",
        "# Define the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes\n",
        "workflow.add_node(\"call_tool_orchestrator\", call_tool_orchestrator)\n",
        "# Use LangGraph's prebuilt ToolNode to execute the tools\n",
        "tool_executor = ToolNode(tools) # Pass the list of StructuredTools here\n",
        "workflow.add_node(\"call_tool_executor\", tool_executor)\n",
        "workflow.add_node(\"generate_strategic_analysis\", generate_strategic_analysis)\n",
        "\n",
        "# Define edges\n",
        "workflow.set_entry_point(\"call_tool_orchestrator\")\n",
        "\n",
        "# If `call_tool_orchestrator` successfully determines a tool to call, go to the executor\n",
        "# Add a conditional edge here from tool orchestrator\n",
        "def route_tool_orchestrator(state: AgentState) -> Literal[\"call_tool_executor\", \"generate_strategic_analysis\"]:\n",
        "    \"\"\"\n",
        "    Determines the next step after tool orchestration.\n",
        "    If a tool was chosen, execute it. Otherwise, proceed to analysis.\n",
        "    \"\"\"\n",
        "    if state.get('tool_to_execute'):\n",
        "        return \"call_tool_executor\"\n",
        "    else:\n",
        "        return \"generate_strategic_analysis\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"call_tool_orchestrator\",\n",
        "    route_tool_orchestrator\n",
        ")\n",
        "\n",
        "\n",
        "# After the tool is executed, go to the strategic analysis generation\n",
        "workflow.add_edge(\"call_tool_executor\", \"generate_strategic_analysis\")\n",
        "\n",
        "# After generating the analysis, the process ends\n",
        "workflow.add_edge(\"generate_strategic_analysis\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "\n",
        "async def run_strategic_analysis(input_json: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Main function to run the strategic analysis pipeline.\n",
        "    Expects input_json conforming to FrontendInput schema.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        frontend_input = FrontendInput(**input_json)\n",
        "    except ValidationError as e:\n",
        "        raise ValueError(f\"Invalid input data: {e.errors()}\")\n",
        "\n",
        "    # Initialize data manager with the current company\n",
        "    data_manager.set_current_company(frontend_input.current_company_id)\n",
        "    print(f\"--- Starting Analysis for Objective: {frontend_input.strategic_objective.value} (Company: {frontend_input.current_company_id}) ---\")\n",
        "\n",
        "    initial_state: AgentState = {\n",
        "        \"input\": frontend_input,\n",
        "        \"current_company_id\": frontend_input.current_company_id,\n",
        "        \"chat_history\": [HumanMessage(content=f\"Initiating strategic analysis for objective: {frontend_input.strategic_objective.value}\")],\n",
        "        \"strategic_objective\": frontend_input.strategic_objective,\n",
        "        \"custom_query_text\": frontend_input.custom_query_text,\n",
        "        \"filters\": frontend_input.filters,\n",
        "        \"scratchpad\": \"Starting analysis...\",\n",
        "        \"recommendations\": [],\n",
        "        \"final_output\": None\n",
        "    }\n",
        "\n",
        "    # Run the graph\n",
        "    final_state = {}\n",
        "    async for state in app.astream(initial_state):\n",
        "        if \"__end__\" not in state:\n",
        "            # print(state) # Print intermediate states for debugging if needed\n",
        "            # For ToolNode output, we need to extract relevant part for scratchpad\n",
        "            if 'tool_output' in state: # ToolNode places output in 'tool_output' key by default\n",
        "                tool_result = state['tool_output']\n",
        "                tool_output_str = \"\"\n",
        "                if isinstance(tool_result, list):\n",
        "                    # Attempt to handle list of Pydantic models by dumping them\n",
        "                    try:\n",
        "                        tool_output_str = json.dumps([item.model_dump() if hasattr(item, 'model_dump') else item for item in tool_result], indent=2)\n",
        "                    except TypeError: # Fallback for non-serializable items\n",
        "                        tool_output_str = str(tool_result)\n",
        "                elif isinstance(tool_result, BaseModel): # Handle single Pydantic model\n",
        "                    tool_output_str = json.dumps(tool_result.model_dump(), indent=2)\n",
        "                elif isinstance(tool_result, dict):\n",
        "                    tool_output_str = json.dumps(tool_result, indent=2)\n",
        "                else:\n",
        "                    tool_output_str = str(tool_result)\n",
        "\n",
        "                state['scratchpad'] = (\n",
        "                    state.get('scratchpad', '') + f\"\\nTool Output ({state.get('tool_executed', 'UnknownTool')}): {tool_output_str}\"\n",
        "                )\n",
        "                state['chat_history'].append(AIMessage(content=f\"Tool Output: {tool_output_str}\"))\n",
        "            final_state.update(state) # Accumulate state\n",
        "        else:\n",
        "            final_state = state[\"__end__\"] # Get the final state\n",
        "\n",
        "    # The final_output will be in final_state['final_output']\n",
        "    return final_state.get(\"final_output\", {}).model_dump() if final_state.get(\"final_output\") else {\"error\": \"Analysis did not produce a valid output.\"}\n",
        "\n",
        "\n",
        "# --- 7. Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    async def main():\n",
        "        # Example 1: Pruning the Portfolio for InnovatePharma Inc.\n",
        "        pruning_input_json = {\n",
        "            \"filters\": {\n",
        "                \"for_sale\": \"all\",\n",
        "                \"asset_type\": \"Molecule\",\n",
        "                \"asset_phenotype\": {\n",
        "                    \"stages_of_development\": [\"Phase 2\", \"Phase 3\", \"Marketed\"],\n",
        "                    \"therapeutic_area\": [\"Metabolic\", \"Oncology\", \"Dermatology\"]\n",
        "                }\n",
        "            },\n",
        "            \"strategic_objective\": \"Pruning the Portfolio\",\n",
        "            # CORRECTED: Using the correct company ID from your JSON\n",
        "            \"current_company_id\": \"YOUR_COMPANY_001\"\n",
        "        }\n",
        "\n",
        "        print(\"\\n--- Running Pruning Analysis ---\")\n",
        "        try:\n",
        "            pruning_output = await run_strategic_analysis(pruning_input_json)\n",
        "            print(\"\\nPruning Output:\")\n",
        "            # Use json.dumps to pretty print the output\n",
        "            print(json.dumps(pruning_output, indent=2))\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during strategic analysis: {e}\")\n",
        "\n",
        "    await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJkO8bYVBbuJ",
        "outputId": "31ae2566-9305-408e-debd-b96210d5c58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 6 companies.\n",
            "Loaded 11 molecules.\n",
            "Loaded 3 external trial data entries.\n",
            "Loaded 4 market benchmarks.\n",
            "Loaded 5 literature snippets.\n",
            "\n",
            "--- Running Pruning Analysis ---\n",
            "--- Starting Analysis for Objective: Pruning the Portfolio (Company: YOUR_COMPANY_001) ---\n",
            "---CALLING TOOL ORCHESTRATOR (LLM DECIDING)---\n",
            "LLM decided to call: search_internal_portfolio(filters={'for_sale': 'all', 'company_details': {'headquarters': None, 'development_stage': None, 'territory': None, 'company_type': None, 'financial_status': None, 'partner_status': None}, 'search_query': None, 'deal_value_max': None, 'peak_sales': {'one_yr_sales_potential_M': None, 'peak_sales_M': None, 'five_yr_sales_potential_M': None}, 'asset_phenotype': {'route_of_administration': None, 'stages_of_development': ['Phase 2', 'Phase 3', 'Marketed'], 'modality': None, 'therapeutic_area': ['Metabolic', 'Oncology', 'Dermatology'], 'patent_expiry_yr': None, 'indication': None, 'mechanism_of_action': None}, 'deal_value_min': None, 'asset_type': 'Molecule'})\n",
            "---GENERATING STRATEGIC ANALYSIS---\n",
            "Error generating strategic analysis or processing tool call: LLM did not call the 'submit_strategic_analysis' tool.\n",
            "Raw LLM message: content='' additional_kwargs={'function_call': {'name': 'search_internal_portfolio', 'arguments': '{\"filters\": {\"for_sale\": \"all\", \"company_details\": {\"headquarters\": null, \"development_stage\": null, \"territory\": null, \"company_type\": null, \"financial_status\": null, \"partner_status\": null}, \"search_query\": null, \"deal_value_max\": null, \"peak_sales\": {\"one_yr_sales_potential_M\": null, \"peak_sales_M\": null, \"five_yr_sales_potential_M\": null}, \"asset_phenotype\": {\"route_of_administration\": null, \"stages_of_development\": [\"Phase 2\", \"Phase 3\", \"Marketed\"], \"modality\": null, \"therapeutic_area\": [\"Metabolic\", \"Oncology\", \"Dermatology\"], \"indication\": null, \"patent_expiry_yr\": null, \"mechanism_of_action\": null}, \"deal_value_min\": null, \"asset_type\": \"Molecule\"}}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--923d6931-cea3-49a0-b037-1b15ba7efd04-0' tool_calls=[{'name': 'search_internal_portfolio', 'args': {'filters': {'for_sale': 'all', 'company_details': {'headquarters': None, 'development_stage': None, 'territory': None, 'company_type': None, 'financial_status': None, 'partner_status': None}, 'search_query': None, 'deal_value_max': None, 'peak_sales': {'one_yr_sales_potential_M': None, 'peak_sales_M': None, 'five_yr_sales_potential_M': None}, 'asset_phenotype': {'route_of_administration': None, 'stages_of_development': ['Phase 2', 'Phase 3', 'Marketed'], 'modality': None, 'therapeutic_area': ['Metabolic', 'Oncology', 'Dermatology'], 'indication': None, 'patent_expiry_yr': None, 'mechanism_of_action': None}, 'deal_value_min': None, 'asset_type': 'Molecule'}}, 'id': 'b43eb6ce-410d-4338-ade0-16393da9807e', 'type': 'tool_call'}] usage_metadata={'input_tokens': 4283, 'output_tokens': 132, 'total_tokens': 4415, 'input_token_details': {'cache_read': 0}}\n",
            "\n",
            "Pruning Output:\n",
            "{\n",
            "  \"error\": \"Analysis did not produce a valid output.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional, Literal, TypedDict\n",
        "from pydantic import BaseModel, Field, conint, confloat, ValidationError\n",
        "from enum import Enum\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Union\n",
        "\n",
        "# LangChain specific imports\n",
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# --- API Keys and Configuration ---\n",
        "# Ensure your GOOGLE_API_KEY is correctly set.\n",
        "# It's recommended to use environment variables for security.\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
        "GOOGLE_API_KEY = \"AIzaSyC4q-ry8oPTjBHDP1suYrtB2PX52MXREwg\" # Replace with your key\n",
        "\n",
        "# Optional, for Langsmith tracing\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"Strategic Portfolio AI\"\n",
        "\n",
        "# --- 1. Enums and Pydantic Models ---\n",
        "class AssetType(str, Enum): MOLECULE = \"Molecule\"\n",
        "class StrategicObjectiveEnum(str, Enum): PRUNING = \"Pruning the Portfolio\"\n",
        "class Molecule(BaseModel): id: str; name: str; asset_type: AssetType; development_stage: str; therapeutic_area: str; indication: str; mechanism_of_action: str; route_of_administration: str; modality: str; patent_expiry_year: int; projected_peak_sales_M: Optional[confloat(ge=0)] = None; internal_risk_score: confloat(ge=0, le=1); efficacy_profile: str; safety_profile: str; company_id: str; company_name: str\n",
        "class AssetPhenotypeFilters(BaseModel): stages_of_development: Optional[List[str]] = None\n",
        "class FrontendFilters(BaseModel): asset_phenotype: Optional[AssetPhenotypeFilters] = Field(default_factory=AssetPhenotypeFilters)\n",
        "class FrontendInput(BaseModel): filters: FrontendFilters = Field(default_factory=FrontendFilters); strategic_objective: StrategicObjectiveEnum; current_company_id: str\n",
        "class PruningRecommendation(BaseModel): action_type: Literal[\"Deprioritize Asset\"]; molecule_name: str; molecule_id: str; justification: str; reason_criteria: List[str]; risk_score: confloat(ge=0, le=1); opportunity_cost_estimate: str; impact_on_portfolio: Dict[str, Any]\n",
        "class StrategicOutput(BaseModel): strategic_objective_addressed: StrategicObjectiveEnum; recommendations: List[PruningRecommendation] = Field(default_factory=list); strategic_summary: str; overall_impact_on_portfolio: Dict[str, Any]; recommended_portfolio_adjustments: Dict[str, Any]; suggested_ideal_portfolio_characteristics: Dict[str, Any]\n",
        "\n",
        "# --- Initialize Global LLM ---\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1, google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# --- Data Management Layer ---\n",
        "class DataManager:\n",
        "    # ... (This class is correct and remains unchanged) ...\n",
        "    def __init__(self, data_file_path: str): self._load_all_data(data_file_path)\n",
        "    def _load_all_data(self, data_file_path):\n",
        "        try:\n",
        "            with open(data_file_path, 'r') as f: data = json.load(f)\n",
        "            self.all_molecules: Dict[str, Molecule] = {m[\"id\"]: Molecule(**m) for m in data.get(\"molecules\", [])}\n",
        "            print(f\"Loaded {len(self.all_molecules)} molecules.\")\n",
        "            self.your_company_id: Optional[str] = None; self.internal_molecules: Dict[str, Molecule] = {}\n",
        "        except Exception as e: raise e\n",
        "    def set_current_company(self, company_id: str): self.your_company_id = company_id; self.internal_molecules = {mol.id: mol for mol in self.all_molecules.values() if mol.company_id == company_id}\n",
        "    def get_internal_portfolio(self) -> List[Molecule]: return list(self.internal_molecules.values())\n",
        "    def get_filtered_molecules(self, molecules: List[Molecule], filters: FrontendFilters) -> List[Molecule]:\n",
        "        if not filters.asset_phenotype or not filters.asset_phenotype.stages_of_development: return molecules\n",
        "        return [m for m in molecules if m.development_stage in filters.asset_phenotype.stages_of_development]\n",
        "\n",
        "if not os.path.exists(\"company.json\"):\n",
        "    dummy_data = {\"molecules\": [{\"id\": \"MOL_001\", \"name\": \"Innovatinib\", \"asset_type\": \"Molecule\", \"development_stage\": \"Phase 2\", \"therapeutic_area\": \"Oncology\", \"indication\": \"NSCLC\", \"mechanism_of_action\": \"EGFR inhibitor\", \"route_of_administration\": \"Oral\", \"modality\": \"Small Molecule\", \"patent_expiry_year\": 2035, \"projected_peak_sales_M\": 1200, \"internal_risk_score\": 0.4, \"efficacy_profile\": \"Promising\", \"safety_profile\": \"Manageable\", \"company_id\": \"YOUR_COMPANY_001\", \"company_name\": \"InnovatePharma Inc.\"}, {\"id\": \"MOL_002\", \"name\": \"Dermacure\", \"asset_type\": \"Molecule\", \"development_stage\": \"Phase 1\", \"therapeutic_area\": \"Dermatology\", \"indication\": \"Psoriasis\", \"mechanism_of_action\": \"JAK inhibitor\", \"route_of_administration\": \"Topical\", \"modality\": \"Small Molecule\", \"patent_expiry_year\": 2038, \"projected_peak_sales_M\": 800, \"internal_risk_score\": 0.6, \"efficacy_profile\": \"Early but positive\", \"safety_profile\": \"Good\", \"company_id\": \"YOUR_COMPANY_001\", \"company_name\": \"InnovatePharma Inc.\"}]}\n",
        "    with open(\"company.json\", \"w\") as f: json.dump(dummy_data, f, indent=2)\n",
        "\n",
        "data_manager = DataManager('company.json')\n",
        "\n",
        "# --- Tool Definitions ---\n",
        "# Note: We are removing the submit_strategic_analysis tool as the final node will handle output.\n",
        "@tool\n",
        "def search_internal_portfolio(filters: FrontendFilters) -> List[Molecule]:\n",
        "    \"\"\"Searches the current company's internal molecule portfolio using structured filters.\"\"\"\n",
        "    print(f\"Tool 'search_internal_portfolio' called with filters: {filters.model_dump_json(indent=2)}\")\n",
        "    all_internal = data_manager.get_internal_portfolio()\n",
        "    return data_manager.get_filtered_molecules(all_internal, filters)\n",
        "\n",
        "tools = [search_internal_portfolio]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# --- Agent State and Graph Definition ---\n",
        "class AgentState(TypedDict):\n",
        "    input: FrontendInput\n",
        "    chat_history: List[BaseMessage]\n",
        "    scratchpad: str\n",
        "    final_output: Optional[StrategicOutput]\n",
        "\n",
        "# Node 1: The \"Orchestrator\" - decides which data-gathering tool to call\n",
        "def call_tool_orchestrator(state: AgentState):\n",
        "    \"\"\"This node uses the LLM to decide which data-gathering tool to call.\"\"\"\n",
        "    print(\"---NODE: call_tool_orchestrator---\")\n",
        "    messages = state['chat_history']\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=\"You are an expert strategic analyst. Your current task is to gather data by calling the most relevant tool.\"),\n",
        "        HumanMessage(content=f\"Based on the strategic objective '{state['input'].strategic_objective.value}', what tool should I call first? The user has provided these filters: {state['input'].filters.model_dump_json(indent=2)}\")\n",
        "    ])\n",
        "    chain = prompt | llm_with_tools\n",
        "    try:\n",
        "        ai_message = chain.invoke({})\n",
        "        if not ai_message.tool_calls:\n",
        "            print(\"Orchestrator chose not to call a tool. Proceeding to final analysis.\")\n",
        "        return {\"chat_history\": messages + [ai_message]}\n",
        "    except Exception as e:\n",
        "        print(f\"Error in tool orchestration: {e}\")\n",
        "        return {\"chat_history\": messages + [AIMessage(content=f\"Error: {e}\")]}\n",
        "\n",
        "# Node 2: The \"Doer\" - executes the tool call\n",
        "tool_executor = ToolExecutor(tools)\n",
        "def execute_tools_node(state: AgentState):\n",
        "    \"\"\"A wrapper node that executes tools.\"\"\"\n",
        "    print(\"---NODE: execute_tools_node---\")\n",
        "    last_message = state['chat_history'][-1]\n",
        "    tool_call = last_message.tool_calls[0]\n",
        "    action = ToolInvocation(tool=tool_call[\"name\"], tool_input=tool_call[\"args\"])\n",
        "    response = tool_executor.invoke([action]) # response is a list of ToolMessage\n",
        "    return {\"chat_history\": state['chat_history'] + response}\n",
        "\n",
        "\n",
        "# Node 3: The \"Processor\" - formats tool output for the scratchpad\n",
        "def process_tool_results(state: AgentState):\n",
        "    \"\"\"Processes the output from the tool node and updates the scratchpad.\"\"\"\n",
        "    print(\"---NODE: process_tool_results---\")\n",
        "    last_message = state['chat_history'][-1]\n",
        "    if not isinstance(last_message, ToolMessage): return {}\n",
        "\n",
        "    try:\n",
        "        # The content of the ToolMessage is now a list of Pydantic objects.\n",
        "        tool_output_list = last_message.content\n",
        "        if isinstance(tool_output_list, list):\n",
        "            serializable_list = [item.model_dump() for item in tool_output_list]\n",
        "            tool_output_str = json.dumps(serializable_list, indent=2)\n",
        "        else:\n",
        "            tool_output_str = str(tool_output_list)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not parse tool output: {e}\")\n",
        "        tool_output_str = str(last_message.content)\n",
        "\n",
        "    updated_scratchpad = state['scratchpad'] + f\"\\n\\n### Data from `{last_message.name}`:\\n```json\\n{tool_output_str}\\n```\\n\"\n",
        "    return {\"scratchpad\": updated_scratchpad}\n",
        "\n",
        "\n",
        "# Node 4: The \"Generator\" - creates the final JSON output\n",
        "def generate_strategic_analysis(state: AgentState):\n",
        "    \"\"\"Generates the final structured JSON output based on the scratchpad.\"\"\"\n",
        "    print(\"---NODE: generate_strategic_analysis---\")\n",
        "    parser = PydanticOutputParser(pydantic_object=StrategicOutput)\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=f\"\"\"You are a world-class strategic portfolio analyst. Your task is to synthesize the provided data into a final, structured JSON report. The user's strategic objective is '{state['input'].strategic_objective.value}'. You must strictly follow the provided JSON format instructions.\"\"\"),\n",
        "        HumanMessage(content=\"Here is the data you have gathered in the scratchpad:\\n{scratchpad}\\n\\nNow, generate the final JSON report based on this data.\\n{format_instructions}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt_template | llm | parser\n",
        "\n",
        "    try:\n",
        "        output = chain.invoke({\n",
        "            \"scratchpad\": state['scratchpad'],\n",
        "            \"format_instructions\": parser.get_format_instructions()\n",
        "        })\n",
        "        return {\"final_output\": output}\n",
        "    except Exception as e:\n",
        "        print(f\"Error during final analysis generation: {e}\")\n",
        "        return {\"final_output\": None}\n",
        "\n",
        "\n",
        "# --- Define and Compile the Graph ---\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"orchestrator\", call_tool_orchestrator)\n",
        "workflow.add_node(\"executor\", execute_tools_node)\n",
        "workflow.add_node(\"processor\", process_tool_results)\n",
        "workflow.add_node(\"generator\", generate_strategic_analysis)\n",
        "\n",
        "def route_from_orchestrator(state: AgentState):\n",
        "    last_message = state['chat_history'][-1]\n",
        "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
        "        return \"executor\" # If a tool was chosen, execute it\n",
        "    return \"generator\" # Otherwise, go straight to the final report\n",
        "\n",
        "workflow.set_entry_point(\"orchestrator\")\n",
        "workflow.add_conditional_edges(\"orchestrator\", route_from_orchestrator)\n",
        "workflow.add_edge(\"executor\", \"processor\")\n",
        "workflow.add_edge(\"processor\", \"generator\")\n",
        "workflow.add_edge(\"generator\", END)\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- Runner Function ---\n",
        "async def run_strategic_analysis(input_json: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    try:\n",
        "        frontend_input = FrontendInput(**input_json)\n",
        "    except ValidationError as e:\n",
        "        return {\"error\": f\"Invalid input data: {e.errors()}\"}\n",
        "\n",
        "    data_manager.set_current_company(frontend_input.current_company_id)\n",
        "    print(f\"--- Starting Analysis for Objective: {frontend_input.strategic_objective.value} (Company: {frontend_input.current_company_id}) ---\")\n",
        "\n",
        "    initial_state = {\n",
        "        \"input\": frontend_input,\n",
        "        \"chat_history\": [],\n",
        "        \"scratchpad\": \"## Initial Analysis State\\n- Objective: \" + frontend_input.strategic_objective.value,\n",
        "        \"final_output\": None\n",
        "    }\n",
        "\n",
        "    final_state = {}\n",
        "    async for output in app.astream(initial_state):\n",
        "        if END in output:\n",
        "            final_state = output[END]\n",
        "            break\n",
        "\n",
        "    output_obj = final_state.get(\"final_output\")\n",
        "    if output_obj and isinstance(output_obj, StrategicOutput):\n",
        "        return output_obj.model_dump()\n",
        "    else:\n",
        "        print(\"\\n--- ANALYSIS FAILED ---\")\n",
        "        return {\"error\": \"Analysis failed to produce a valid StrategicOutput. See server logs.\"}\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    async def main():\n",
        "        pruning_input_json = {\n",
        "            \"filters\": {\n",
        "                \"asset_phenotype\": {\n",
        "                    \"stages_of_development\": [\"Phase 1\", \"Phase 2\"]\n",
        "                }\n",
        "            },\n",
        "            \"strategic_objective\": \"Pruning the Portfolio\",\n",
        "            \"current_company_id\": \"YOUR_COMPANY_001\"\n",
        "        }\n",
        "        print(\"\\n--- Running Pruning Analysis ---\")\n",
        "        try:\n",
        "            pruning_output = await run_strategic_analysis(pruning_input_json)\n",
        "            print(\"\\n<<< FINAL STRATEGIC OUTPUT >>>\")\n",
        "            print(json.dumps(pruning_output, indent=2))\n",
        "        except Exception as e:\n",
        "            print(f\"An unhandled error occurred during the analysis run: {e}\")\n",
        "\n",
        "    await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "XK0bz1KpKffs",
        "outputId": "faa85707-3e98-49e5-d497-79931381753b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 11 molecules.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ToolExecutor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-43-1889114246.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Node 2: The \"Doer\" - executes the tool call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mtool_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute_tools_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAgentState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m\"\"\"A wrapper node that executes tools.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ToolExecutor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional, Literal, TypedDict\n",
        "from pydantic import BaseModel, Field, conint, confloat, ValidationError\n",
        "from enum import Enum\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Union\n",
        "\n",
        "# LangChain specific imports\n",
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# --- API Keys and Configuration ---\n",
        "# It's highly recommended to use environment variables for security.\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
        "GOOGLE_API_KEY = \"AIzaSyC4q-ry8oPTjBHDP1suYrtB2PX52MXREwg\" # Replace with your key\n",
        "\n",
        "# Optional, for Langsmith tracing\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"Strategic Portfolio AI\"\n",
        "\n",
        "\n",
        "class AssetType(str, Enum): MOLECULE = \"Molecule\"\n",
        "class StrategicObjectiveEnum(str, Enum): PRUNING = \"Pruning the Portfolio\"\n",
        "class Molecule(BaseModel): id: str; name: str; asset_type: AssetType; development_stage: str; therapeutic_area: str; indication: str; mechanism_of_action: str; route_of_administration: str; modality: str; patent_expiry_year: int; projected_peak_sales_M: Optional[confloat(ge=0)] = None; internal_risk_score: confloat(ge=0, le=1); efficacy_profile: str; safety_profile: str; company_id: str; company_name: str\n",
        "class AssetPhenotypeFilters(BaseModel): stages_of_development: Optional[List[str]] = None\n",
        "class FrontendFilters(BaseModel): asset_phenotype: Optional[AssetPhenotypeFilters] = Field(default_factory=AssetPhenotypeFilters)\n",
        "class FrontendInput(BaseModel): filters: FrontendFilters = Field(default_factory=FrontendFilters); strategic_objective: StrategicObjectiveEnum; current_company_id: str\n",
        "class PruningRecommendation(BaseModel): action_type: Literal[\"Deprioritize Asset\"]; molecule_name: str; molecule_id: str; justification: str; reason_criteria: List[str]; risk_score: confloat(ge=0, le=1); opportunity_cost_estimate: str; impact_on_portfolio: Dict[str, Any]\n",
        "class StrategicOutput(BaseModel): strategic_objective_addressed: StrategicObjectiveEnum; recommendations: List[PruningRecommendation] = Field(default_factory=list); strategic_summary: str; overall_impact_on_portfolio: Dict[str, Any]; recommended_portfolio_adjustments: Dict[str, Any]; suggested_ideal_portfolio_characteristics: Dict[str, Any]\n",
        "\n",
        "# --- Initialize Global LLM ---\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1, google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# --- Data Management Layer ---\n",
        "class DataManager:\n",
        "    def __init__(self, data_file_path: str): self._load_all_data(data_file_path)\n",
        "    def _load_all_data(self, data_file_path):\n",
        "        try:\n",
        "            with open(data_file_path, 'r') as f: data = json.load(f)\n",
        "            self.all_molecules: Dict[str, Molecule] = {m[\"id\"]: Molecule(**m) for m in data.get(\"molecules\", [])}\n",
        "            print(f\"Loaded {len(self.all_molecules)} molecules.\")\n",
        "            self.your_company_id: Optional[str] = None; self.internal_molecules: Dict[str, Molecule] = {}\n",
        "        except Exception as e: raise e\n",
        "    def set_current_company(self, company_id: str): self.your_company_id = company_id; self.internal_molecules = {mol.id: mol for mol in self.all_molecules.values() if mol.company_id == company_id}\n",
        "    def get_internal_portfolio(self) -> List[Molecule]: return list(self.internal_molecules.values())\n",
        "    def get_filtered_molecules(self, molecules: List[Molecule], filters: FrontendFilters) -> List[Molecule]:\n",
        "        if not filters.asset_phenotype or not filters.asset_phenotype.stages_of_development: return molecules\n",
        "        return [m for m in molecules if m.development_stage in filters.asset_phenotype.stages_of_development]\n",
        "\n",
        "if not os.path.exists(\"company.json\"):\n",
        "    dummy_data = {\"molecules\": [{\"id\": \"MOL_001\", \"name\": \"Innovatinib\", \"asset_type\": \"Molecule\", \"development_stage\": \"Phase 2\", \"therapeutic_area\": \"Oncology\", \"indication\": \"NSCLC\", \"mechanism_of_action\": \"EGFR inhibitor\", \"route_of_administration\": \"Oral\", \"modality\": \"Small Molecule\", \"patent_expiry_year\": 2035, \"projected_peak_sales_M\": 1200, \"internal_risk_score\": 0.4, \"efficacy_profile\": \"Promising\", \"safety_profile\": \"Manageable\", \"company_id\": \"YOUR_COMPANY_001\", \"company_name\": \"InnovatePharma Inc.\"}, {\"id\": \"MOL_002\", \"name\": \"Dermacure\", \"asset_type\": \"Molecule\", \"development_stage\": \"Phase 1\", \"therapeutic_area\": \"Dermatology\", \"indication\": \"Psoriasis\", \"mechanism_of_action\": \"JAK inhibitor\", \"route_of_administration\": \"Topical\", \"modality\": \"Small Molecule\", \"patent_expiry_year\": 2038, \"projected_peak_sales_M\": 800, \"internal_risk_score\": 0.6, \"efficacy_profile\": \"Early but positive\", \"safety_profile\": \"Good\", \"company_id\": \"YOUR_COMPANY_001\", \"company_name\": \"InnovatePharma Inc.\"}]}\n",
        "    with open(\"company.json\", \"w\") as f: json.dump(dummy_data, f, indent=2)\n",
        "\n",
        "data_manager = DataManager('company.json')\n",
        "\n",
        "# --- Tool Definitions ---\n",
        "@tool\n",
        "def search_internal_portfolio(filters: FrontendFilters) -> List[Molecule]:\n",
        "    \"\"\"Searches the current company's internal molecule portfolio using structured filters.\"\"\"\n",
        "    print(f\"Tool 'search_internal_portfolio' called with filters: {filters.model_dump_json(indent=2)}\")\n",
        "    all_internal = data_manager.get_internal_portfolio()\n",
        "    return data_manager.get_filtered_molecules(all_internal, filters)\n",
        "\n",
        "@tool\n",
        "def submit_strategic_analysis(analysis: StrategicOutput) -> str:\n",
        "    \"\"\"Use this tool to submit the final strategic portfolio analysis and recommendations. This is the final step.\"\"\"\n",
        "    print(\"Tool 'submit_strategic_analysis' called.\")\n",
        "    return \"Strategic analysis submitted successfully. The process is complete.\"\n",
        "\n",
        "tools = [search_internal_portfolio, submit_strategic_analysis]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# --- Agent State and Graph Definition ---\n",
        "class AgentState(TypedDict):\n",
        "    input: FrontendInput\n",
        "    chat_history: List[BaseMessage]\n",
        "    final_output: Optional[StrategicOutput]\n",
        "\n",
        "# Node 1: The \"thinker\"\n",
        "def agent_node(state: AgentState):\n",
        "    \"\"\"Invokes the LLM to decide the next action or to finish.\"\"\"\n",
        "    print(\"---AGENT: Thinking...---\")\n",
        "    response = llm_with_tools.invoke(state[\"chat_history\"])\n",
        "    return {\"chat_history\": state[\"chat_history\"] + [response]}\n",
        "\n",
        "# Node 2: The \"doer\" - a WRAPPER around ToolNode\n",
        "tool_node_instance = ToolNode(tools)\n",
        "def execute_tools_node(state: AgentState) -> Dict[str, List[BaseMessage]]:\n",
        "    \"\"\"A wrapper node that executes tools by invoking the ToolNode with the chat history.\"\"\"\n",
        "    print(\"---TOOLS: Executing...---\")\n",
        "    messages = state['chat_history']\n",
        "    tool_invocation_result = tool_node_instance.invoke(messages)\n",
        "    if not isinstance(tool_invocation_result, list):\n",
        "        tool_invocation_result = [tool_invocation_result]\n",
        "    return {\"chat_history\": state[\"chat_history\"] + tool_invocation_result}\n",
        "\n",
        "# Conditional Edge: The Router\n",
        "def should_continue(state: AgentState) -> Literal[\"execute_tools_node\", END]:\n",
        "    \"\"\"Checks the last message in the state to decide the next step.\"\"\"\n",
        "    print(\"---ROUTER: Checking state---\")\n",
        "    last_message = state[\"chat_history\"][-1]\n",
        "    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
        "        print(\"-> LLM did not call a tool. Ending.\")\n",
        "        return END\n",
        "    if last_message.tool_calls[0][\"name\"] == \"submit_strategic_analysis\":\n",
        "        try:\n",
        "            final_data = last_message.tool_calls[0]['args']['analysis']\n",
        "            state['final_output'] = StrategicOutput(**final_data)\n",
        "            print(\"-> 'submit_strategic_analysis' called. Ending.\")\n",
        "        except (ValidationError, KeyError) as e:\n",
        "            print(f\"-> ERROR: Could not parse final analysis. {e}. Ending.\")\n",
        "        return END\n",
        "    print(\"-> Tool call detected. Continuing to executor.\")\n",
        "    return \"execute_tools_node\"\n",
        "\n",
        "# --- Define and Compile the Graph ---\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"agent\", agent_node)\n",
        "workflow.add_node(\"execute_tools_node\", execute_tools_node)\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.add_conditional_edges(\"agent\", should_continue, {\"execute_tools_node\": \"execute_tools_node\", END: END})\n",
        "workflow.add_edge(\"execute_tools_node\", \"agent\")\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- Runner Function ---\n",
        "async def run_strategic_analysis(input_json: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    try:\n",
        "        frontend_input = FrontendInput(**input_json)\n",
        "    except ValidationError as e:\n",
        "        return {\"error\": f\"Invalid input data: {e.errors()}\"}\n",
        "\n",
        "    data_manager.set_current_company(frontend_input.current_company_id)\n",
        "    print(f\"--- Starting Analysis for Objective: {frontend_input.strategic_objective.value} (Company: {frontend_input.current_company_id}) ---\")\n",
        "\n",
        "    # <<< FIX: Use a persistent SystemMessage for core instructions >>>\n",
        "    system_prompt = f\"\"\"You are an expert strategic portfolio analyst. Your job is to execute a multi-step analysis using your available tools.\n",
        "\n",
        "    Your overall goal is to address the user's strategic objective: \"{frontend_input.strategic_objective.value}\".\n",
        "\n",
        "    Follow this process:\n",
        "    1.  Your first step is ALWAYS to call the `search_internal_portfolio` tool to get the data you need to analyze.\n",
        "    2.  After you receive the portfolio data from the tool, you MUST analyze it.\n",
        "    3.  Based on your analysis, your final step is to call the `submit_strategic_analysis` tool with your complete findings. Do not respond with simple text; you must finish by calling this tool.\"\"\"\n",
        "\n",
        "    # The HumanMessage now only provides the specific data for the task\n",
        "    human_prompt = f\"\"\"Please begin the analysis. Here are the filters to use for the `search_internal_portfolio` tool:\n",
        "    {frontend_input.filters.model_dump_json(indent=2)}\n",
        "    \"\"\"\n",
        "\n",
        "    initial_state = {\n",
        "        \"input\": frontend_input,\n",
        "        \"chat_history\": [\n",
        "            SystemMessage(content=system_prompt),\n",
        "            HumanMessage(content=human_prompt)\n",
        "        ],\n",
        "        \"final_output\": None\n",
        "    }\n",
        "\n",
        "    final_state = {}\n",
        "    async for output in app.astream(initial_state):\n",
        "        if END in output:\n",
        "            final_state = output[END]\n",
        "            break\n",
        "\n",
        "    output_obj = final_state.get(\"final_output\")\n",
        "\n",
        "    if output_obj and isinstance(output_obj, StrategicOutput):\n",
        "        return output_obj.model_dump()\n",
        "    else:\n",
        "        print(\"\\n--- ANALYSIS FAILED ---\")\n",
        "        if final_state and 'chat_history' in final_state:\n",
        "            print(\"\\n--- Final Chat History ---\")\n",
        "            for msg in final_state['chat_history']:\n",
        "                print(\"-------------\")\n",
        "                print(type(msg))\n",
        "                print(msg.content)\n",
        "            print(\"------------------------\")\n",
        "        return {\"error\": \"Analysis failed to produce a valid StrategicOutput. See server logs.\"}\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    async def main():\n",
        "        pruning_input_json = {\"filters\": {\"for_sale\": \"all\", \"asset_type\": \"Molecule\", \"asset_phenotype\": {\"stages_of_development\": [\"Phase 2\", \"Phase 3\", \"Marketed\"], \"therapeutic_area\": [\"Metabolic\", \"Oncology\", \"Dermatology\"]}}, \"strategic_objective\": \"Pruning the Portfolio\", \"current_company_id\": \"YOUR_COMPANY_001\"}\n",
        "\n",
        "        print(\"\\n--- Running Pruning Analysis ---\")\n",
        "        try:\n",
        "            pruning_output = await run_strategic_analysis(pruning_input_json)\n",
        "            print(\"\\n<<< FINAL STRATEGIC OUTPUT >>>\")\n",
        "            print(json.dumps(pruning_output, indent=2))\n",
        "        except Exception as e:\n",
        "            print(f\"An unhandled error occurred during the analysis run: {e}\")\n",
        "    await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kI_F3UPNbU7",
        "outputId": "32c50d0b-d883-4992-beb5-86f255953fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 11 molecules.\n",
            "\n",
            "--- Running Pruning Analysis ---\n",
            "--- Starting Analysis for Objective: Pruning the Portfolio (Company: YOUR_COMPANY_001) ---\n",
            "---AGENT: Thinking...---\n",
            "---ROUTER: Checking state---\n",
            "-> Tool call detected. Continuing to executor.\n",
            "---TOOLS: Executing...---\n",
            "---AGENT: Thinking...---\n",
            "---ROUTER: Checking state---\n",
            "-> Tool call detected. Continuing to executor.\n",
            "---TOOLS: Executing...---\n",
            "Tool 'search_internal_portfolio' called with filters: {\n",
            "  \"asset_phenotype\": {\n",
            "    \"stages_of_development\": [\n",
            "      \"Phase 2\",\n",
            "      \"Phase 3\",\n",
            "      \"Marketed\"\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "---AGENT: Thinking...---\n",
            "---ROUTER: Checking state---\n",
            "-> ERROR: Could not parse final analysis. 5 validation errors for StrategicOutput\n",
            "recommendations.0.action_type\n",
            "  Input should be 'Deprioritize Asset' [type=literal_error, input_value='Discontinue', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\n",
            "recommendations.0.impact_on_portfolio\n",
            "  Input should be a valid dictionary [type=dict_type, input_value='Reduced risk and resource allocation', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/dict_type\n",
            "overall_impact_on_portfolio\n",
            "  Input should be a valid dictionary [type=dict_type, input_value='Streamlined Portfolio', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/dict_type\n",
            "recommended_portfolio_adjustments\n",
            "  Input should be a valid dictionary [type=dict_type, input_value='Focus resources on higher potential assets', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/dict_type\n",
            "suggested_ideal_portfolio_characteristics\n",
            "  Input should be a valid dictionary [type=dict_type, input_value='High projected sales, lo...acy and safety profiles', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/dict_type. Ending.\n",
            "\n",
            "--- ANALYSIS FAILED ---\n",
            "\n",
            "<<< FINAL STRATEGIC OUTPUT >>>\n",
            "{\n",
            "  \"error\": \"Analysis failed to produce a valid StrategicOutput. See server logs.\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}